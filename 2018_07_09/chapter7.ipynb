{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ４次元配列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高さ28・横幅28で1チャンネルのデータが10個ある場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(10, 1, 28, 28) # ランダムにデータを生成\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各データへのアクセス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape # (1, 28, 28)\n",
    "x[1].shape # (1, 28, 28) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データの1チャンネル目の空間データにアクセス（28×28の数値データ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97020214, 0.91463794, 0.43741578, 0.15137601, 0.28266198,\n",
       "        0.79503946, 0.60324284, 0.35155465, 0.83660588, 0.35181556,\n",
       "        0.44480022, 0.53954861, 0.99450197, 0.18998135, 0.381965  ,\n",
       "        0.22610977, 0.8328797 , 0.15387826, 0.81959077, 0.73081432,\n",
       "        0.82835099, 0.06053113, 0.22591576, 0.811113  , 0.42295761,\n",
       "        0.35073203, 0.88570232, 0.28550079],\n",
       "       [0.36862906, 0.42621914, 0.54707829, 0.87292   , 0.55280619,\n",
       "        0.26964344, 0.61508364, 0.9147639 , 0.14081584, 0.01225429,\n",
       "        0.8753357 , 0.97815308, 0.21906155, 0.2068474 , 0.21159562,\n",
       "        0.24419141, 0.37600386, 0.6768068 , 0.64740537, 0.79461506,\n",
       "        0.07423679, 0.1664924 , 0.586301  , 0.97812874, 0.71345373,\n",
       "        0.22550259, 0.43047251, 0.73351226],\n",
       "       [0.93184966, 0.39514571, 0.48941687, 0.29380096, 0.12512171,\n",
       "        0.25203819, 0.66227238, 0.34089919, 0.98810685, 0.82135633,\n",
       "        0.9450434 , 0.89598144, 0.34320045, 0.82652542, 0.08998041,\n",
       "        0.46527965, 0.39749819, 0.62249482, 0.47729066, 0.0546849 ,\n",
       "        0.38114522, 0.00787706, 0.64312297, 0.76454858, 0.56633197,\n",
       "        0.07194078, 0.41029786, 0.65183819],\n",
       "       [0.10857901, 0.74130887, 0.88780985, 0.19379554, 0.64574403,\n",
       "        0.26230756, 0.2485739 , 0.18843061, 0.68334989, 0.10452244,\n",
       "        0.8342419 , 0.6757094 , 0.59824738, 0.31479469, 0.31777074,\n",
       "        0.04123207, 0.06207638, 0.8205259 , 0.45532852, 0.3193633 ,\n",
       "        0.30714947, 0.4973941 , 0.94462822, 0.59949383, 0.41733826,\n",
       "        0.10549562, 0.26424761, 0.30211571],\n",
       "       [0.48973928, 0.65830935, 0.3963095 , 0.40092308, 0.25902294,\n",
       "        0.7749495 , 0.47583051, 0.06459896, 0.69519328, 0.34094857,\n",
       "        0.61102418, 0.29277719, 0.04137853, 0.52454836, 0.54395474,\n",
       "        0.17915687, 0.33941285, 0.55344139, 0.40822998, 0.92737666,\n",
       "        0.9881414 , 0.34875423, 0.0669935 , 0.80387346, 0.05066258,\n",
       "        0.00733085, 0.66827765, 0.64318269],\n",
       "       [0.54334064, 0.5080228 , 0.85330097, 0.64866227, 0.27595827,\n",
       "        0.35355536, 0.08244583, 0.16101493, 0.4826004 , 0.69250032,\n",
       "        0.08052449, 0.2633948 , 0.38121181, 0.29267641, 0.81341926,\n",
       "        0.67382454, 0.44645228, 0.30274423, 0.89413054, 0.59714001,\n",
       "        0.75520353, 0.94300509, 0.46540489, 0.76632346, 0.87213854,\n",
       "        0.9604532 , 0.63624235, 0.44595913],\n",
       "       [0.64447658, 0.3340272 , 0.45098595, 0.24945787, 0.11143072,\n",
       "        0.73193213, 0.19187749, 0.27814136, 0.67732908, 0.43139856,\n",
       "        0.2669713 , 0.78595687, 0.75276192, 0.32327897, 0.32271096,\n",
       "        0.31460898, 0.53684336, 0.0228011 , 0.8079311 , 0.31294049,\n",
       "        0.89441459, 0.59913986, 0.31253275, 0.91178294, 0.01601708,\n",
       "        0.19668089, 0.95329673, 0.43941756],\n",
       "       [0.13996263, 0.82003879, 0.0970236 , 0.80969912, 0.36750389,\n",
       "        0.36640556, 0.12012768, 0.88318942, 0.9661931 , 0.10512935,\n",
       "        0.24012568, 0.81699029, 0.72690143, 0.42915066, 0.63711454,\n",
       "        0.43150502, 0.26701717, 0.70637686, 0.33134776, 0.69010623,\n",
       "        0.42772832, 0.6633241 , 0.48394261, 0.38187444, 0.89736855,\n",
       "        0.15786486, 0.33070843, 0.20538343],\n",
       "       [0.09317748, 0.14532128, 0.23865441, 0.4806176 , 0.21968698,\n",
       "        0.57718083, 0.93604184, 0.47481142, 0.37193446, 0.05899989,\n",
       "        0.04370202, 0.76940894, 0.39231327, 0.58041625, 0.05178716,\n",
       "        0.87757066, 0.55518338, 0.08924992, 0.30612066, 0.87140494,\n",
       "        0.99234156, 0.26916273, 0.23084274, 0.89979912, 0.68865383,\n",
       "        0.30740565, 0.83025386, 0.4948022 ],\n",
       "       [0.23818393, 0.69852569, 0.447607  , 0.5129743 , 0.23235416,\n",
       "        0.68934556, 0.84135466, 0.09752795, 0.40816462, 0.61943396,\n",
       "        0.90934292, 0.48397674, 0.78580611, 0.93351708, 0.23744849,\n",
       "        0.5881775 , 0.69044829, 0.38854121, 0.54887363, 0.13194952,\n",
       "        0.36148582, 0.65083621, 0.81777528, 0.9046978 , 0.88828026,\n",
       "        0.47566836, 0.88229683, 0.05180255],\n",
       "       [0.21484574, 0.31552339, 0.81998916, 0.17984832, 0.13398521,\n",
       "        0.1286425 , 0.60884693, 0.11657337, 0.07236365, 0.26643954,\n",
       "        0.31400806, 0.00679526, 0.96981667, 0.92688081, 0.63233785,\n",
       "        0.32585334, 0.132548  , 0.86905412, 0.29217827, 0.57495278,\n",
       "        0.86863199, 0.0688848 , 0.26308026, 0.03074892, 0.17564094,\n",
       "        0.68188829, 0.94507438, 0.55751585],\n",
       "       [0.77543309, 0.71954077, 0.37542071, 0.91095864, 0.73181738,\n",
       "        0.70026593, 0.85389311, 0.39570675, 0.46889922, 0.50561922,\n",
       "        0.68424934, 0.22123712, 0.5843646 , 0.57664147, 0.54618033,\n",
       "        0.08525533, 0.67004198, 0.25730404, 0.81171147, 0.52254001,\n",
       "        0.21469865, 0.77675208, 0.42722497, 0.25512582, 0.53600489,\n",
       "        0.88758605, 0.45845721, 0.57240194],\n",
       "       [0.61390768, 0.28773615, 0.26497458, 0.48296221, 0.24948394,\n",
       "        0.65249242, 0.1157287 , 0.56769637, 0.89025005, 0.42076498,\n",
       "        0.91195094, 0.93486515, 0.45312618, 0.70935054, 0.19607591,\n",
       "        0.75456563, 0.5321148 , 0.65094864, 0.49843938, 0.92457164,\n",
       "        0.94488573, 0.93519073, 0.91346291, 0.71934709, 0.6683519 ,\n",
       "        0.02237506, 0.26744105, 0.67260935],\n",
       "       [0.71587738, 0.13035476, 0.45452698, 0.03380352, 0.47486055,\n",
       "        0.25299962, 0.84818458, 0.46701916, 0.28031544, 0.7006686 ,\n",
       "        0.45155699, 0.50822797, 0.27535157, 0.97321072, 0.27192837,\n",
       "        0.65048124, 0.52713771, 0.54501885, 0.89594009, 0.16217987,\n",
       "        0.63513016, 0.33263465, 0.77179967, 0.82608288, 0.23106674,\n",
       "        0.92375379, 0.25340871, 0.60448335],\n",
       "       [0.4431816 , 0.36459121, 0.01305032, 0.40500383, 0.55452154,\n",
       "        0.52572742, 0.19036588, 0.10925027, 0.63668229, 0.74648143,\n",
       "        0.78923954, 0.22848365, 0.8454759 , 0.25943475, 0.97283018,\n",
       "        0.75142352, 0.65562344, 0.65400706, 0.48534423, 0.16212675,\n",
       "        0.26470705, 0.09569209, 0.62355897, 0.78857279, 0.9760176 ,\n",
       "        0.90809583, 0.73063511, 0.01690423],\n",
       "       [0.31627202, 0.0697923 , 0.70661338, 0.0456423 , 0.48212397,\n",
       "        0.79897638, 0.81645015, 0.66940888, 0.94908871, 0.43672502,\n",
       "        0.88096023, 0.74965708, 0.6029517 , 0.12944328, 0.88306417,\n",
       "        0.56822171, 0.4389374 , 0.87572565, 0.53844303, 0.95857733,\n",
       "        0.62603807, 0.57307261, 0.82532409, 0.95941663, 0.36310043,\n",
       "        0.01737045, 0.57779238, 0.64515397],\n",
       "       [0.14162995, 0.82131955, 0.58873065, 0.87511009, 0.32272027,\n",
       "        0.77878107, 0.0429153 , 0.59112853, 0.82525801, 0.24191966,\n",
       "        0.48711018, 0.42836624, 0.01035013, 0.34423324, 0.18217533,\n",
       "        0.03545646, 0.35467964, 0.07002554, 0.86680532, 0.25393273,\n",
       "        0.9384373 , 0.74876839, 0.69175985, 0.67821559, 0.86739262,\n",
       "        0.5008356 , 0.95994927, 0.63887585],\n",
       "       [0.96169273, 0.04694976, 0.76538257, 0.74558414, 0.75083514,\n",
       "        0.06077652, 0.71082544, 0.62562446, 0.67809682, 0.71736683,\n",
       "        0.85979464, 0.94655196, 0.26281237, 0.68800407, 0.96761691,\n",
       "        0.48369197, 0.83287324, 0.29709732, 0.9520479 , 0.05169829,\n",
       "        0.47714727, 0.87971594, 0.96658949, 0.40930047, 0.3267828 ,\n",
       "        0.4562525 , 0.42779774, 0.49447069],\n",
       "       [0.74199071, 0.66602874, 0.95760531, 0.1584522 , 0.33845963,\n",
       "        0.04010119, 0.37640269, 0.02448348, 0.12072142, 0.12018712,\n",
       "        0.88217151, 0.65686508, 0.11015528, 0.91729738, 0.5413215 ,\n",
       "        0.56377179, 0.03357936, 0.13473854, 0.57811216, 0.71965576,\n",
       "        0.30039214, 0.16161438, 0.60553188, 0.88262934, 0.86543569,\n",
       "        0.40306535, 0.80150906, 0.04152152],\n",
       "       [0.22811853, 0.2011053 , 0.1069099 , 0.34565944, 0.43573849,\n",
       "        0.97448287, 0.31390011, 0.28890222, 0.07248338, 0.83127292,\n",
       "        0.34656646, 0.17052246, 0.50945894, 0.92735823, 0.11112558,\n",
       "        0.98281279, 0.07996144, 0.03382387, 0.55878916, 0.95705093,\n",
       "        0.15371679, 0.70907568, 0.00786596, 0.98632985, 0.10544881,\n",
       "        0.96651357, 0.04569664, 0.02303102],\n",
       "       [0.2963871 , 0.71138437, 0.05794732, 0.26962475, 0.26885379,\n",
       "        0.87687551, 0.25573958, 0.477272  , 0.67977315, 0.37488148,\n",
       "        0.12419745, 0.04537565, 0.50411116, 0.64815226, 0.05229002,\n",
       "        0.90170989, 0.53408925, 0.50851035, 0.49304689, 0.79843064,\n",
       "        0.33788725, 0.06374568, 0.05998545, 0.36491939, 0.1193343 ,\n",
       "        0.75705087, 0.33833188, 0.93975056],\n",
       "       [0.52637842, 0.62102553, 0.97630769, 0.89542848, 0.47065916,\n",
       "        0.04214396, 0.53404637, 0.77875737, 0.75203254, 0.0235248 ,\n",
       "        0.82361965, 0.93349433, 0.81749718, 0.87841143, 0.94789184,\n",
       "        0.5883612 , 0.61297339, 0.82616529, 0.79597208, 0.22844054,\n",
       "        0.60094293, 0.67386824, 0.21023876, 0.64972939, 0.68473306,\n",
       "        0.82246223, 0.62004699, 0.42423446],\n",
       "       [0.28206424, 0.57886148, 0.20415899, 0.52663721, 0.80405104,\n",
       "        0.32244401, 0.66635383, 0.79510685, 0.6446859 , 0.870066  ,\n",
       "        0.47656904, 0.00328101, 0.70178646, 0.34818958, 0.70173052,\n",
       "        0.56852673, 0.5529798 , 0.6688952 , 0.24273728, 0.26183264,\n",
       "        0.95492163, 0.17337869, 0.25306134, 0.90003737, 0.52863039,\n",
       "        0.79310174, 0.2684768 , 0.7236022 ],\n",
       "       [0.04379889, 0.25881431, 0.87874895, 0.5923114 , 0.72757299,\n",
       "        0.28023038, 0.30771433, 0.93556002, 0.77602446, 0.31593038,\n",
       "        0.33508631, 0.44622665, 0.69427294, 0.91204135, 0.00256497,\n",
       "        0.58487557, 0.6285751 , 0.48861824, 0.99451276, 0.79234036,\n",
       "        0.48732902, 0.78774638, 0.95104019, 0.35611128, 0.13449143,\n",
       "        0.7324634 , 0.7876165 , 0.23765504],\n",
       "       [0.39991206, 0.29323148, 0.85695541, 0.64677568, 0.02488967,\n",
       "        0.587014  , 0.75870762, 0.22830177, 0.25148446, 0.39534191,\n",
       "        0.76692716, 0.94822247, 0.86196811, 0.75079739, 0.86058492,\n",
       "        0.74136396, 0.75491864, 0.87003604, 0.20496193, 0.63804634,\n",
       "        0.52748608, 0.39929838, 0.88524577, 0.32098447, 0.48349188,\n",
       "        0.12902722, 0.54133182, 0.73955175],\n",
       "       [0.15327708, 0.52298907, 0.89367854, 0.91461118, 0.96926398,\n",
       "        0.00495742, 0.11788562, 0.49567759, 0.02198437, 0.02257141,\n",
       "        0.17488703, 0.45048793, 0.0875598 , 0.6942706 , 0.52660509,\n",
       "        0.65472574, 0.09657002, 0.52668914, 0.94762607, 0.04809431,\n",
       "        0.58521966, 0.21678987, 0.97198683, 0.95700435, 0.9676806 ,\n",
       "        0.76715809, 0.73058175, 0.25589161],\n",
       "       [0.59968216, 0.15907901, 0.64949975, 0.61719381, 0.73591094,\n",
       "        0.13534258, 0.83612274, 0.10213703, 0.35199338, 0.44780818,\n",
       "        0.82655901, 0.91365261, 0.05868919, 0.79231115, 0.87460943,\n",
       "        0.4270011 , 0.40668738, 0.47655842, 0.54515563, 0.57283691,\n",
       "        0.19811658, 0.77146639, 0.75715769, 0.99434197, 0.92713812,\n",
       "        0.62045688, 0.05207813, 0.25117044],\n",
       "       [0.2219467 , 0.95142648, 0.3270455 , 0.38847308, 0.37727944,\n",
       "        0.92600437, 0.16174384, 0.16027534, 0.10533862, 0.38896112,\n",
       "        0.00827056, 0.08847177, 0.52643407, 0.07683306, 0.77640054,\n",
       "        0.36732766, 0.17810574, 0.94781934, 0.18763041, 0.30401627,\n",
       "        0.0628827 , 0.79413662, 0.46481738, 0.08499197, 0.22209504,\n",
       "        0.59106473, 0.05092199, 0.67225367]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0] # もしくは x[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データ数 N=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7) # 10 個のデータ\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畳み込み層の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # フィルターの展開\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プーリング層の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "       \n",
    "        # 展開 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # 最大値 (2)\n",
    "        out = np.max(col, axis=1)\n",
    "        # 整形 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNISTデータセットの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2994001891194764\n",
      "=== epoch:1, train acc:0.13, test acc:0.159 ===\n",
      "train loss:2.297814015954517\n",
      "train loss:2.2950924509586317\n",
      "train loss:2.2885327193721046\n",
      "train loss:2.2799573250780822\n",
      "train loss:2.2640110831953155\n",
      "train loss:2.261228398848327\n",
      "train loss:2.2365422848433387\n",
      "train loss:2.2294981716165587\n",
      "train loss:2.1950187134375385\n",
      "train loss:2.1965935627639226\n",
      "train loss:2.1099178414890876\n",
      "train loss:2.1064982888403323\n",
      "train loss:2.054092943180061\n",
      "train loss:1.9632511668897152\n",
      "train loss:1.8963152604247475\n",
      "train loss:1.9150136862532312\n",
      "train loss:1.7894725528971096\n",
      "train loss:1.8013235451000296\n",
      "train loss:1.685221812420436\n",
      "train loss:1.5789070219094612\n",
      "train loss:1.5556490365780857\n",
      "train loss:1.4110977589898988\n",
      "train loss:1.3238883870400004\n",
      "train loss:1.2787565838375603\n",
      "train loss:1.117632479662609\n",
      "train loss:1.149418127709431\n",
      "train loss:0.9561359581505579\n",
      "train loss:1.1105386906726316\n",
      "train loss:1.0872682888583436\n",
      "train loss:0.9150096133998938\n",
      "train loss:0.9315653584141494\n",
      "train loss:0.7600865851334678\n",
      "train loss:0.698773941354165\n",
      "train loss:0.7658660833710853\n",
      "train loss:0.5431915952325226\n",
      "train loss:0.6824097057972941\n",
      "train loss:0.6716179483828985\n",
      "train loss:0.69426549878061\n",
      "train loss:0.6668845581102625\n",
      "train loss:0.8097360818361704\n",
      "train loss:0.7129015008623193\n",
      "train loss:0.8536781497295888\n",
      "train loss:0.6233066402969776\n",
      "train loss:0.7112843576244048\n",
      "train loss:0.7014300485342795\n",
      "train loss:0.48435197288230447\n",
      "train loss:0.6738650053552927\n",
      "train loss:0.6852742825578185\n",
      "train loss:0.7242115983477408\n",
      "train loss:0.595478260241403\n",
      "train loss:0.4978081906722188\n",
      "train loss:0.8619508941504662\n",
      "train loss:0.5073273227069045\n",
      "train loss:0.6287442932193467\n",
      "train loss:0.6611040400148152\n",
      "train loss:0.6268601716394325\n",
      "train loss:0.4695809026998714\n",
      "train loss:0.43164372405886864\n",
      "train loss:0.47906277007951076\n",
      "train loss:0.5825549407843151\n",
      "train loss:0.5804728001476519\n",
      "train loss:0.5323080571854805\n",
      "train loss:0.4203838917693121\n",
      "train loss:0.4386876229749393\n",
      "train loss:0.7372214256697194\n",
      "train loss:0.44713719499745336\n",
      "train loss:0.5994230244044588\n",
      "train loss:0.46448570936475664\n",
      "train loss:0.459552626100611\n",
      "train loss:0.5460282400336807\n",
      "train loss:0.4527459448339459\n",
      "train loss:0.43086372010554713\n",
      "train loss:0.5475450335045133\n",
      "train loss:0.48117340544731446\n",
      "train loss:0.35085240629967207\n",
      "train loss:0.4174425898020799\n",
      "train loss:0.37832220779472714\n",
      "train loss:0.3035290258072105\n",
      "train loss:0.4141387691858425\n",
      "train loss:0.3461754611666122\n",
      "train loss:0.4223155265079708\n",
      "train loss:0.49157185367281964\n",
      "train loss:0.3546148610409098\n",
      "train loss:0.36388334835518277\n",
      "train loss:0.4010268836237896\n",
      "train loss:0.47163439249420114\n",
      "train loss:0.42823200090194363\n",
      "train loss:0.3107371567183592\n",
      "train loss:0.32976117508942493\n",
      "train loss:0.2896290258042317\n",
      "train loss:0.38285966686684453\n",
      "train loss:0.3476382973561231\n",
      "train loss:0.4714528367973829\n",
      "train loss:0.26010135497746245\n",
      "train loss:0.5803491144451118\n",
      "train loss:0.39325603458074626\n",
      "train loss:0.21813647915680368\n",
      "train loss:0.5069219072313301\n",
      "train loss:0.39926806215692534\n",
      "train loss:0.3728401922864057\n",
      "train loss:0.3656225346631715\n",
      "train loss:0.24460583180431542\n",
      "train loss:0.3472121111986705\n",
      "train loss:0.3031562970021033\n",
      "train loss:0.378047963860575\n",
      "train loss:0.26168489149631785\n",
      "train loss:0.3713318982201186\n",
      "train loss:0.23063934572601394\n",
      "train loss:0.2436152045030207\n",
      "train loss:0.39628799322292907\n",
      "train loss:0.3806234277248582\n",
      "train loss:0.28375903159790156\n",
      "train loss:0.24371710669876553\n",
      "train loss:0.28158405856273083\n",
      "train loss:0.6005269376775559\n",
      "train loss:0.20971032013913576\n",
      "train loss:0.2905787179809445\n",
      "train loss:0.38853447375451233\n",
      "train loss:0.18489496914895479\n",
      "train loss:0.3027518435861059\n",
      "train loss:0.242749092773364\n",
      "train loss:0.49562320270444205\n",
      "train loss:0.25880787824330115\n",
      "train loss:0.21691450592327488\n",
      "train loss:0.25605834789122645\n",
      "train loss:0.25757480936932\n",
      "train loss:0.26478183884263395\n",
      "train loss:0.17868044906452896\n",
      "train loss:0.29003705682377406\n",
      "train loss:0.4405532408108474\n",
      "train loss:0.28988435525424067\n",
      "train loss:0.2710409654820991\n",
      "train loss:0.2900119192766708\n",
      "train loss:0.4852544112402182\n",
      "train loss:0.36756013273825233\n",
      "train loss:0.5904582244173909\n",
      "train loss:0.28106673307857016\n",
      "train loss:0.3573908208025876\n",
      "train loss:0.3991386037868261\n",
      "train loss:0.32381126758270606\n",
      "train loss:0.3714134186455968\n",
      "train loss:0.24392895751489366\n",
      "train loss:0.35526389613822323\n",
      "train loss:0.23470564758590606\n",
      "train loss:0.2548133898625624\n",
      "train loss:0.30151509166680235\n",
      "train loss:0.27114923286573095\n",
      "train loss:0.28305302595495707\n",
      "train loss:0.30769813488594494\n",
      "train loss:0.27982175479023036\n",
      "train loss:0.24708981287321585\n",
      "train loss:0.4020821958495803\n",
      "train loss:0.49227992033539747\n",
      "train loss:0.25633189046371085\n",
      "train loss:0.3013061429489215\n",
      "train loss:0.2425110728215478\n",
      "train loss:0.32647624062611064\n",
      "train loss:0.2690533309042736\n",
      "train loss:0.2107713573563816\n",
      "train loss:0.25608781885156134\n",
      "train loss:0.2267083729609438\n",
      "train loss:0.27073734384303355\n",
      "train loss:0.2037482430252249\n",
      "train loss:0.4068209535807277\n",
      "train loss:0.12557206638001076\n",
      "train loss:0.39098641453460586\n",
      "train loss:0.2793862312760835\n",
      "train loss:0.2715300686257141\n",
      "train loss:0.36284400898213187\n",
      "train loss:0.1718136298581615\n",
      "train loss:0.3228156373121436\n",
      "train loss:0.4314265975747849\n",
      "train loss:0.2896846219323055\n",
      "train loss:0.3273482004202737\n",
      "train loss:0.19082411318866493\n",
      "train loss:0.31533924214936804\n",
      "train loss:0.30263901704551605\n",
      "train loss:0.36851009584428973\n",
      "train loss:0.3967198605180375\n",
      "train loss:0.14162341091501834\n",
      "train loss:0.28244298305790627\n",
      "train loss:0.2729933998918057\n",
      "train loss:0.29564215413523537\n",
      "train loss:0.18429438599766254\n",
      "train loss:0.2063295625350666\n",
      "train loss:0.3765995701114063\n",
      "train loss:0.2621728057242605\n",
      "train loss:0.40048908863541927\n",
      "train loss:0.3769359677022179\n",
      "train loss:0.1461490361537252\n",
      "train loss:0.40662295466292614\n",
      "train loss:0.2824939985676745\n",
      "train loss:0.23314366577961324\n",
      "train loss:0.16520322564964665\n",
      "train loss:0.2668821626637712\n",
      "train loss:0.2724052809876656\n",
      "train loss:0.23303264293148745\n",
      "train loss:0.2687063725310371\n",
      "train loss:0.2925334543559625\n",
      "train loss:0.37587629093452507\n",
      "train loss:0.2331474781781575\n",
      "train loss:0.2164220395325875\n",
      "train loss:0.2782550605344567\n",
      "train loss:0.24157964160727455\n",
      "train loss:0.1570391162477698\n",
      "train loss:0.21359121618634358\n",
      "train loss:0.3126470503454593\n",
      "train loss:0.1959624442546487\n",
      "train loss:0.15759574817404876\n",
      "train loss:0.24635674935721125\n",
      "train loss:0.17606419628469755\n",
      "train loss:0.237467747181198\n",
      "train loss:0.17564574483730166\n",
      "train loss:0.13330031754288563\n",
      "train loss:0.29017228237775866\n",
      "train loss:0.18628767547534977\n",
      "train loss:0.12226678632786502\n",
      "train loss:0.3017824662883769\n",
      "train loss:0.14526552881996846\n",
      "train loss:0.24715609472137814\n",
      "train loss:0.23949935163176747\n",
      "train loss:0.4164688590655\n",
      "train loss:0.332963310018891\n",
      "train loss:0.16043437040634956\n",
      "train loss:0.3561230665599261\n",
      "train loss:0.11509181319234284\n",
      "train loss:0.24226915895798687\n",
      "train loss:0.2595123632153632\n",
      "train loss:0.26567647021598434\n",
      "train loss:0.2315483553028043\n",
      "train loss:0.24392095264308722\n",
      "train loss:0.17975857041340298\n",
      "train loss:0.27054643541514994\n",
      "train loss:0.15383209705914014\n",
      "train loss:0.19755330647013833\n",
      "train loss:0.25348266762609195\n",
      "train loss:0.22725163393912642\n",
      "train loss:0.2917080319914553\n",
      "train loss:0.20792561366032658\n",
      "train loss:0.12210314233576218\n",
      "train loss:0.2782018031386349\n",
      "train loss:0.19221286031459925\n",
      "train loss:0.20596769341618562\n",
      "train loss:0.115623046056272\n",
      "train loss:0.21030493632583838\n",
      "train loss:0.1793267877459483\n",
      "train loss:0.28901347791703796\n",
      "train loss:0.1707942443503535\n",
      "train loss:0.4114958802603981\n",
      "train loss:0.2060778786547184\n",
      "train loss:0.22438092207670696\n",
      "train loss:0.1704821780329517\n",
      "train loss:0.21440591452738933\n",
      "train loss:0.30762749236405734\n",
      "train loss:0.17216458265399587\n",
      "train loss:0.24608698425133346\n",
      "train loss:0.2045121084394671\n",
      "train loss:0.2048715630839107\n",
      "train loss:0.10918467440078264\n",
      "train loss:0.23457954014405613\n",
      "train loss:0.2317048547937749\n",
      "train loss:0.20532715308454702\n",
      "train loss:0.2877020678935507\n",
      "train loss:0.2884293130044194\n",
      "train loss:0.3823916704698893\n",
      "train loss:0.2506433550969198\n",
      "train loss:0.2349545390722281\n",
      "train loss:0.11171455301089919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1742728776809988\n",
      "train loss:0.12610204254212173\n",
      "train loss:0.15895788186310555\n",
      "train loss:0.12999552681884416\n",
      "train loss:0.29616475310021867\n",
      "train loss:0.23781786537014502\n",
      "train loss:0.15215144935214367\n",
      "train loss:0.1999396902187986\n",
      "train loss:0.21868169650402325\n",
      "train loss:0.16961061638817868\n",
      "train loss:0.3322010839303076\n",
      "train loss:0.14170837780998496\n",
      "train loss:0.1567882138050212\n",
      "train loss:0.20587788958188571\n",
      "train loss:0.2787076679370055\n",
      "train loss:0.3091801593089792\n",
      "train loss:0.30391584071397704\n",
      "train loss:0.09611251656736192\n",
      "train loss:0.25916063784076604\n",
      "train loss:0.27155509374110226\n",
      "train loss:0.23004212291150597\n",
      "train loss:0.3230843583274114\n",
      "train loss:0.13451558485154\n",
      "train loss:0.13074046340618437\n",
      "train loss:0.1935799132597757\n",
      "train loss:0.21270017305198954\n",
      "train loss:0.2176885598647622\n",
      "train loss:0.13008134744984073\n",
      "train loss:0.23974257536515967\n",
      "train loss:0.2583583624371088\n",
      "train loss:0.2627641201738847\n",
      "train loss:0.36721097083581905\n",
      "train loss:0.2646487334381797\n",
      "train loss:0.16113176826996323\n",
      "train loss:0.34507185856292966\n",
      "train loss:0.13656436275056172\n",
      "train loss:0.1850047551573192\n",
      "train loss:0.1160000681647337\n",
      "train loss:0.1603817789851087\n",
      "train loss:0.14713409412835735\n",
      "train loss:0.17575121592019133\n",
      "train loss:0.18933646961067344\n",
      "train loss:0.11593561360718584\n",
      "train loss:0.2017521434385246\n",
      "train loss:0.29735198642239147\n",
      "train loss:0.1870466783064146\n",
      "train loss:0.1703083864617289\n",
      "train loss:0.15903079985525337\n",
      "train loss:0.17378058921246553\n",
      "train loss:0.09392138715171973\n",
      "train loss:0.28723801814655187\n",
      "train loss:0.20567680506307176\n",
      "train loss:0.18260927976508642\n",
      "train loss:0.11947682800802738\n",
      "train loss:0.2022453507228855\n",
      "train loss:0.13146355928773962\n",
      "train loss:0.15836646437854016\n",
      "train loss:0.15701830657969762\n",
      "train loss:0.2819287128668478\n",
      "train loss:0.28871564788231713\n",
      "train loss:0.24212952481418648\n",
      "train loss:0.2471976758188263\n",
      "train loss:0.1525487998245234\n",
      "train loss:0.2010809063766299\n",
      "train loss:0.16383567780066866\n",
      "train loss:0.19762468991395596\n",
      "train loss:0.16847162038946936\n",
      "train loss:0.1421616536051146\n",
      "train loss:0.16244765840046027\n",
      "train loss:0.12929871931822554\n",
      "train loss:0.1626291945805839\n",
      "train loss:0.09187007805580494\n",
      "train loss:0.30239405840690803\n",
      "train loss:0.07099432403490083\n",
      "train loss:0.1214143663020827\n",
      "train loss:0.27541648251621476\n",
      "train loss:0.1587224130749824\n",
      "train loss:0.309148252794351\n",
      "train loss:0.18585482644675033\n",
      "train loss:0.1635146950890998\n",
      "train loss:0.1773471714222511\n",
      "train loss:0.2581656516194782\n",
      "train loss:0.1224035992700945\n",
      "train loss:0.12481133298830721\n",
      "train loss:0.09847307053436954\n",
      "train loss:0.16302302730584603\n",
      "train loss:0.08627751326701892\n",
      "train loss:0.08746185321224409\n",
      "train loss:0.17651830496540055\n",
      "train loss:0.153151539821105\n",
      "train loss:0.28415284582530254\n",
      "train loss:0.1339240237366468\n",
      "train loss:0.12513643694457915\n",
      "train loss:0.1381968061768447\n",
      "train loss:0.13304864495164817\n",
      "train loss:0.2692535719896517\n",
      "train loss:0.23314365666270312\n",
      "train loss:0.13575690064004856\n",
      "train loss:0.12069111075467809\n",
      "train loss:0.21395420560366918\n",
      "train loss:0.22156871666282577\n",
      "train loss:0.10114843187217179\n",
      "train loss:0.2500359786653357\n",
      "train loss:0.1655776395315895\n",
      "train loss:0.2794015681706854\n",
      "train loss:0.32097919597554864\n",
      "train loss:0.26206326304274247\n",
      "train loss:0.08142764516950049\n",
      "train loss:0.2443211751346843\n",
      "train loss:0.10031581222145251\n",
      "train loss:0.12800046928150188\n",
      "train loss:0.26529696249684553\n",
      "train loss:0.0916854335159512\n",
      "train loss:0.14639655747841662\n",
      "train loss:0.1146165028501502\n",
      "train loss:0.1520540936841757\n",
      "train loss:0.08674215245048156\n",
      "train loss:0.14769046195890026\n",
      "train loss:0.1686745879322428\n",
      "train loss:0.10217758017412144\n",
      "train loss:0.11819504150691618\n",
      "train loss:0.09170267882647101\n",
      "train loss:0.08577106012461032\n",
      "train loss:0.13226021013852654\n",
      "train loss:0.08296559589788402\n",
      "train loss:0.15853460926045806\n",
      "train loss:0.10040015635716903\n",
      "train loss:0.18593768727833887\n",
      "train loss:0.17564592140645355\n",
      "train loss:0.16280765058901317\n",
      "train loss:0.24426666024943156\n",
      "train loss:0.09505373712218432\n",
      "train loss:0.08803246202448882\n",
      "train loss:0.2204671508663534\n",
      "train loss:0.08726902526723364\n",
      "train loss:0.1355626596656804\n",
      "train loss:0.12215441544020603\n",
      "train loss:0.11778621810821836\n",
      "train loss:0.11493449547997775\n",
      "train loss:0.07688460233828277\n",
      "train loss:0.20224465782420292\n",
      "train loss:0.133672323520897\n",
      "train loss:0.10028022591466965\n",
      "train loss:0.1439959981306792\n",
      "train loss:0.1377720480991266\n",
      "train loss:0.14735730683246986\n",
      "train loss:0.21300976335842647\n",
      "train loss:0.09582906625892885\n",
      "train loss:0.12803286645318313\n",
      "train loss:0.05685017754562764\n",
      "train loss:0.13857377115461\n",
      "train loss:0.14844816988126838\n",
      "train loss:0.11811596012116073\n",
      "train loss:0.07619210513993148\n",
      "train loss:0.21434143166990974\n",
      "train loss:0.08752452983143748\n",
      "train loss:0.32035680526722116\n",
      "train loss:0.07267751222907569\n",
      "train loss:0.16072822788325303\n",
      "train loss:0.21246868442950664\n",
      "train loss:0.18785679732776697\n",
      "train loss:0.11728607927505572\n",
      "train loss:0.1267234082200941\n",
      "train loss:0.1505021587178667\n",
      "train loss:0.1308866532865239\n",
      "train loss:0.07968319693435003\n",
      "train loss:0.14946873558564291\n",
      "train loss:0.07287522815024713\n",
      "train loss:0.11996409566808745\n",
      "train loss:0.16106248975787507\n",
      "train loss:0.17814114860959054\n",
      "train loss:0.10309602251244084\n",
      "train loss:0.19511093200664295\n",
      "train loss:0.08240635037874916\n",
      "train loss:0.09055137833678188\n",
      "train loss:0.2545569427825604\n",
      "train loss:0.08192693500439133\n",
      "train loss:0.20089506433173873\n",
      "train loss:0.16802751522962855\n",
      "train loss:0.16198946205389803\n",
      "train loss:0.18172384554306031\n",
      "train loss:0.05847625902067162\n",
      "train loss:0.1302310734141092\n",
      "train loss:0.12910609358350672\n",
      "train loss:0.05581811782999331\n",
      "train loss:0.14362723333026334\n",
      "train loss:0.14110776249541088\n",
      "train loss:0.10181304158202632\n",
      "train loss:0.06418158665128652\n",
      "train loss:0.20033593471439778\n",
      "train loss:0.13037353424004633\n",
      "train loss:0.07952759520049905\n",
      "train loss:0.11442599149483396\n",
      "train loss:0.13461254589797003\n",
      "train loss:0.08491651602271783\n",
      "train loss:0.18729456840772835\n",
      "train loss:0.0739688322712487\n",
      "train loss:0.20457405009664734\n",
      "train loss:0.09003730834542671\n",
      "train loss:0.1161068154021542\n",
      "train loss:0.1466765192009597\n",
      "train loss:0.10425474065322674\n",
      "train loss:0.18620775351216537\n",
      "train loss:0.0746164818027232\n",
      "train loss:0.1308662492763582\n",
      "train loss:0.1437724139587834\n",
      "train loss:0.08770429405171778\n",
      "train loss:0.07762434551032958\n",
      "train loss:0.1657052489600805\n",
      "train loss:0.15144556611038149\n",
      "train loss:0.11608210228318291\n",
      "train loss:0.10486958382450495\n",
      "train loss:0.06617975234839588\n",
      "train loss:0.09878552356432037\n",
      "train loss:0.2579509124120354\n",
      "train loss:0.12977490133583808\n",
      "train loss:0.033241483627558625\n",
      "train loss:0.1150672627909827\n",
      "train loss:0.09044097600884392\n",
      "train loss:0.15227558932510976\n",
      "train loss:0.09089947071551574\n",
      "train loss:0.09456063317630634\n",
      "train loss:0.10171549257222415\n",
      "train loss:0.09382347877120348\n",
      "train loss:0.1390727716977432\n",
      "train loss:0.1284232961662327\n",
      "train loss:0.14468066022511258\n",
      "train loss:0.125228863074828\n",
      "train loss:0.09356834459317595\n",
      "train loss:0.12892295873485168\n",
      "train loss:0.17109695605279604\n",
      "train loss:0.22385466522060024\n",
      "train loss:0.10107604491356685\n",
      "train loss:0.07219643331287436\n",
      "train loss:0.08567855376525386\n",
      "train loss:0.16481906662750123\n",
      "train loss:0.10189149607857623\n",
      "train loss:0.14780980508486705\n",
      "train loss:0.07656676590761104\n",
      "train loss:0.14946141694813198\n",
      "train loss:0.1655873887168667\n",
      "train loss:0.32439220126985496\n",
      "train loss:0.07337877845754708\n",
      "train loss:0.1737491100331306\n",
      "train loss:0.14944220763969146\n",
      "train loss:0.08934779969882051\n",
      "train loss:0.07413650908233498\n",
      "train loss:0.1634306252522513\n",
      "train loss:0.11338453143322554\n",
      "train loss:0.12200959360995275\n",
      "train loss:0.1441456885247314\n",
      "train loss:0.09653740930212054\n",
      "train loss:0.1011445038529663\n",
      "train loss:0.04962715265923871\n",
      "train loss:0.0861019693594399\n",
      "train loss:0.13567492356319666\n",
      "train loss:0.11851663193088693\n",
      "train loss:0.13792790213470701\n",
      "train loss:0.08857292770217151\n",
      "train loss:0.15010031193261397\n",
      "train loss:0.02936930617083338\n",
      "train loss:0.11828639145487854\n",
      "train loss:0.11536325884870678\n",
      "train loss:0.15842424974896527\n",
      "train loss:0.08564319261915947\n",
      "train loss:0.12102960882206772\n",
      "train loss:0.1300567697674701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.18327274436193386\n",
      "train loss:0.2008103823714942\n",
      "train loss:0.06466493489224781\n",
      "train loss:0.11417520028340655\n",
      "train loss:0.11105587336318815\n",
      "train loss:0.09630032992598286\n",
      "train loss:0.13488842079980587\n",
      "train loss:0.14401782185619347\n",
      "train loss:0.07911488986668357\n",
      "train loss:0.1121754829827635\n",
      "train loss:0.19281051479230038\n",
      "train loss:0.06999416410450564\n",
      "train loss:0.1540666239990426\n",
      "train loss:0.14563985302722696\n",
      "train loss:0.026330984000739487\n",
      "train loss:0.15345883411059635\n",
      "train loss:0.15810617404647148\n",
      "train loss:0.13047374499906875\n",
      "train loss:0.15610379754766973\n",
      "train loss:0.15613670748825703\n",
      "train loss:0.10559231973211186\n",
      "train loss:0.14163655885622284\n",
      "train loss:0.1089986202266242\n",
      "train loss:0.17185515620445277\n",
      "train loss:0.073571455927178\n",
      "train loss:0.08703362999830098\n",
      "train loss:0.10900117596932937\n",
      "train loss:0.09664317781414629\n",
      "train loss:0.11812193043053558\n",
      "train loss:0.15709523609748413\n",
      "train loss:0.0444238642583138\n",
      "train loss:0.0641929485931773\n",
      "train loss:0.09911341504493139\n",
      "train loss:0.0941233049156448\n",
      "train loss:0.09900453336792343\n",
      "train loss:0.14632104983249494\n",
      "train loss:0.13047109339317525\n",
      "train loss:0.04743520372049915\n",
      "train loss:0.03514085951008441\n",
      "train loss:0.13357359704456606\n",
      "train loss:0.1488913514668277\n",
      "train loss:0.12724320115405346\n",
      "train loss:0.06902887269288885\n",
      "train loss:0.08717163190660598\n",
      "train loss:0.11375810520661649\n",
      "train loss:0.07709892902276767\n",
      "train loss:0.06654976975026186\n",
      "train loss:0.11189970592878071\n",
      "train loss:0.09130179392187446\n",
      "train loss:0.09939916216488927\n",
      "train loss:0.08869499303359162\n",
      "train loss:0.11475030974571172\n",
      "train loss:0.05271747431044293\n",
      "train loss:0.05829455253342171\n",
      "train loss:0.04744309413254017\n",
      "train loss:0.2717334742619594\n",
      "train loss:0.04754784423095465\n",
      "train loss:0.07125771993897528\n",
      "train loss:0.048346822353814\n",
      "train loss:0.09671558045598712\n",
      "train loss:0.09229949797687352\n",
      "train loss:0.04272098423550539\n",
      "train loss:0.032422114559955924\n",
      "train loss:0.08952335524324148\n",
      "train loss:0.1151161015733746\n",
      "=== epoch:2, train acc:0.966, test acc:0.97 ===\n",
      "train loss:0.06505585031860298\n",
      "train loss:0.04826804122622952\n",
      "train loss:0.04270347053567127\n",
      "train loss:0.03814814642961968\n",
      "train loss:0.10549751833054198\n",
      "train loss:0.03757707391331142\n",
      "train loss:0.06028517996064953\n",
      "train loss:0.04858168466351256\n",
      "train loss:0.04268487988367248\n",
      "train loss:0.15586993615467906\n",
      "train loss:0.15520748552492283\n",
      "train loss:0.13512326529494648\n",
      "train loss:0.03712581683969621\n",
      "train loss:0.06811435267911292\n",
      "train loss:0.06717899764000505\n",
      "train loss:0.09337139531135945\n",
      "train loss:0.18105595678976882\n",
      "train loss:0.07379556335071574\n",
      "train loss:0.10088890908772873\n",
      "train loss:0.07869173002553702\n",
      "train loss:0.04748280695614812\n",
      "train loss:0.11652648815683576\n",
      "train loss:0.08558994956631583\n",
      "train loss:0.08600424860279839\n",
      "train loss:0.09378742292576585\n",
      "train loss:0.14059074621227766\n",
      "train loss:0.09662531586229064\n",
      "train loss:0.14478824473702112\n",
      "train loss:0.1209703123392522\n",
      "train loss:0.20467921081872587\n",
      "train loss:0.11650848087951962\n",
      "train loss:0.09262659008856128\n",
      "train loss:0.14313431643555607\n",
      "train loss:0.10628521063354306\n",
      "train loss:0.10704571456929647\n",
      "train loss:0.03879438264973\n",
      "train loss:0.12141534165992786\n",
      "train loss:0.1065452196213068\n",
      "train loss:0.15198852247145683\n",
      "train loss:0.07746245676515899\n",
      "train loss:0.05617894994723348\n",
      "train loss:0.0543436030147781\n",
      "train loss:0.036755376676799976\n",
      "train loss:0.06943159794150831\n",
      "train loss:0.02818240624251219\n",
      "train loss:0.07324647634677092\n",
      "train loss:0.11595176333475705\n",
      "train loss:0.0826721765226338\n",
      "train loss:0.05131994284752034\n",
      "train loss:0.054453387516017936\n",
      "train loss:0.1128720728909986\n",
      "train loss:0.0755146426768491\n",
      "train loss:0.12857837721109946\n",
      "train loss:0.17604130552232938\n",
      "train loss:0.05089803666838894\n",
      "train loss:0.11623414221217761\n",
      "train loss:0.1356165389559726\n",
      "train loss:0.11982413649763549\n",
      "train loss:0.10649229016185462\n",
      "train loss:0.12462639973574731\n",
      "train loss:0.10754109143596574\n",
      "train loss:0.14184323300977109\n",
      "train loss:0.15344492953805433\n",
      "train loss:0.107982186445306\n",
      "train loss:0.09093759136337809\n",
      "train loss:0.1962252379637675\n",
      "train loss:0.03195513667472052\n",
      "train loss:0.15816737691638041\n",
      "train loss:0.08441102977114756\n",
      "train loss:0.1173814586011122\n",
      "train loss:0.04964641530373111\n",
      "train loss:0.07001158871851111\n",
      "train loss:0.2160331361116884\n",
      "train loss:0.10929448012252437\n",
      "train loss:0.0905165312227093\n",
      "train loss:0.062027774736804965\n",
      "train loss:0.08471129253252328\n",
      "train loss:0.03592438396290334\n",
      "train loss:0.046447396418381935\n",
      "train loss:0.15440294102667235\n",
      "train loss:0.036865218996181025\n",
      "train loss:0.14504594402120718\n",
      "train loss:0.06348020086818597\n",
      "train loss:0.06172949620212781\n",
      "train loss:0.06185403567688758\n",
      "train loss:0.13679106317087578\n",
      "train loss:0.06295773469268488\n",
      "train loss:0.07584312751455989\n",
      "train loss:0.061736135684676244\n",
      "train loss:0.06851819508586852\n",
      "train loss:0.09947302091760686\n",
      "train loss:0.12336982416541607\n",
      "train loss:0.1011652422519565\n",
      "train loss:0.13178896698317463\n",
      "train loss:0.06630569834398117\n",
      "train loss:0.046955392587341216\n",
      "train loss:0.11179588426049772\n",
      "train loss:0.07020790933275527\n",
      "train loss:0.15290917141134508\n",
      "train loss:0.15061400100255543\n",
      "train loss:0.20873508579475591\n",
      "train loss:0.09180235511334056\n",
      "train loss:0.0660818012280562\n",
      "train loss:0.05996659563094825\n",
      "train loss:0.09948823031379833\n",
      "train loss:0.12415634166762461\n",
      "train loss:0.040070672177754924\n",
      "train loss:0.10608874958716065\n",
      "train loss:0.08610382585758827\n",
      "train loss:0.20311204397501292\n",
      "train loss:0.038057707006990536\n",
      "train loss:0.05818408064023188\n",
      "train loss:0.0613149025156011\n",
      "train loss:0.06343287211787131\n",
      "train loss:0.032191852922278205\n",
      "train loss:0.12695986950646002\n",
      "train loss:0.16758272062136484\n",
      "train loss:0.10220112326095476\n",
      "train loss:0.0491283321682656\n",
      "train loss:0.08059496471825181\n",
      "train loss:0.06502310883370277\n",
      "train loss:0.05684868382801844\n",
      "train loss:0.06307366948701164\n",
      "train loss:0.04988931476753839\n",
      "train loss:0.07205679465436099\n",
      "train loss:0.06430671806091737\n",
      "train loss:0.05430430722652628\n",
      "train loss:0.08706925203575361\n",
      "train loss:0.08992011502739188\n",
      "train loss:0.20327671037064687\n",
      "train loss:0.0702772121927311\n",
      "train loss:0.09454104997081297\n",
      "train loss:0.07855709933723719\n",
      "train loss:0.07661865311317674\n",
      "train loss:0.03355132342075074\n",
      "train loss:0.0812899676558534\n",
      "train loss:0.08601991853054107\n",
      "train loss:0.0881101624171696\n",
      "train loss:0.030265919278210512\n",
      "train loss:0.10312782702745237\n",
      "train loss:0.04325195788303337\n",
      "train loss:0.09447882781724654\n",
      "train loss:0.040932221304544526\n",
      "train loss:0.10010885658347522\n",
      "train loss:0.04904014854525238\n",
      "train loss:0.05918169030410448\n",
      "train loss:0.05358624855373492\n",
      "train loss:0.14518928400775152\n",
      "train loss:0.07275969860992453\n",
      "train loss:0.08269663017870689\n",
      "train loss:0.09010311509636201\n",
      "train loss:0.09744042243580775\n",
      "train loss:0.09674201668107056\n",
      "train loss:0.09654642945099891\n",
      "train loss:0.03556638513427908\n",
      "train loss:0.12120714921499604\n",
      "train loss:0.07739730590614387\n",
      "train loss:0.08367036989853464\n",
      "train loss:0.08493953290846923\n",
      "train loss:0.08007880996343887\n",
      "train loss:0.0899266058029065\n",
      "train loss:0.03965205966865254\n",
      "train loss:0.08364985797741294\n",
      "train loss:0.08917713209333232\n",
      "train loss:0.052100020836923756\n",
      "train loss:0.053446983800015814\n",
      "train loss:0.08791223948888818\n",
      "train loss:0.08409520699349118\n",
      "train loss:0.05042364194580646\n",
      "train loss:0.06480550789377437\n",
      "train loss:0.07564910769617118\n",
      "train loss:0.08392421323053428\n",
      "train loss:0.07917237628707831\n",
      "train loss:0.047738839819097115\n",
      "train loss:0.10293248807011596\n",
      "train loss:0.0671610831815928\n",
      "train loss:0.08971623140744024\n",
      "train loss:0.2184318227456934\n",
      "train loss:0.11131096118589522\n",
      "train loss:0.04039957487026029\n",
      "train loss:0.17496128553387802\n",
      "train loss:0.048730825176251845\n",
      "train loss:0.10851012262390913\n",
      "train loss:0.06145403465314955\n",
      "train loss:0.07824367928800874\n",
      "train loss:0.10775262923196278\n",
      "train loss:0.1812580285790337\n",
      "train loss:0.06315591678364531\n",
      "train loss:0.015000792990897587\n",
      "train loss:0.05009640393690047\n",
      "train loss:0.12773076755773471\n",
      "train loss:0.03506141169478195\n",
      "train loss:0.12811373001969778\n",
      "train loss:0.06008685270031774\n",
      "train loss:0.02155571795270579\n",
      "train loss:0.11172885159012333\n",
      "train loss:0.06954544330658716\n",
      "train loss:0.10122698356253496\n",
      "train loss:0.07119758696125643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08901577977202373\n",
      "train loss:0.08294666020297102\n",
      "train loss:0.03253126702635732\n",
      "train loss:0.08708758268933295\n",
      "train loss:0.0662230703657055\n",
      "train loss:0.12192867993403263\n",
      "train loss:0.09043517853887742\n",
      "train loss:0.0750760417789266\n",
      "train loss:0.036139357853967026\n",
      "train loss:0.07022793395975956\n",
      "train loss:0.03279719383014986\n",
      "train loss:0.0900025461956139\n",
      "train loss:0.15375109521992292\n",
      "train loss:0.04533364454672796\n",
      "train loss:0.17322236832397825\n",
      "train loss:0.058263702405279366\n",
      "train loss:0.046135534879216955\n",
      "train loss:0.09536500110354484\n",
      "train loss:0.037346034959018705\n",
      "train loss:0.050366878519606466\n",
      "train loss:0.08100883321755495\n",
      "train loss:0.07743810110191257\n",
      "train loss:0.049181977666630274\n",
      "train loss:0.07123734316991819\n",
      "train loss:0.07359406546038534\n",
      "train loss:0.029681128594529812\n",
      "train loss:0.0801249209792265\n",
      "train loss:0.044947180149041414\n",
      "train loss:0.06006915272734854\n",
      "train loss:0.06996867567436807\n",
      "train loss:0.09750901139188167\n",
      "train loss:0.05010738606051751\n",
      "train loss:0.03043460669610801\n",
      "train loss:0.15492659998517572\n",
      "train loss:0.11795863023722539\n",
      "train loss:0.015303752453354547\n",
      "train loss:0.07173575771312357\n",
      "train loss:0.07126252592167019\n",
      "train loss:0.08316630691471852\n",
      "train loss:0.023186379428304368\n",
      "train loss:0.07638961688270783\n",
      "train loss:0.06644355115414922\n",
      "train loss:0.04973874742875726\n",
      "train loss:0.1623856448907708\n",
      "train loss:0.1544178484629205\n",
      "train loss:0.11094788199013919\n",
      "train loss:0.04072081582940476\n",
      "train loss:0.30689581811908434\n",
      "train loss:0.05939302996030808\n",
      "train loss:0.06557286206894587\n",
      "train loss:0.1183228457577999\n",
      "train loss:0.053503845509468684\n",
      "train loss:0.14216215408672211\n",
      "train loss:0.05580173892213981\n",
      "train loss:0.07070513108896745\n",
      "train loss:0.086659820661618\n",
      "train loss:0.13133088934857562\n",
      "train loss:0.08092597758896079\n",
      "train loss:0.12476945203924908\n",
      "train loss:0.16496317455479534\n",
      "train loss:0.08942723871284165\n",
      "train loss:0.11467723650353072\n",
      "train loss:0.05991174401322223\n",
      "train loss:0.07178170196422347\n",
      "train loss:0.08763493109359223\n",
      "train loss:0.09892840137465926\n",
      "train loss:0.06039723868713882\n",
      "train loss:0.14163626163033197\n",
      "train loss:0.11409005940947373\n",
      "train loss:0.08038357762356557\n",
      "train loss:0.07835716897908104\n",
      "train loss:0.116689144449201\n",
      "train loss:0.04777349019450198\n",
      "train loss:0.06252556717246728\n",
      "train loss:0.11925515871411202\n",
      "train loss:0.033415541310758\n",
      "train loss:0.07523822730417191\n",
      "train loss:0.04738331597952006\n",
      "train loss:0.036222804561679076\n",
      "train loss:0.10173312875273936\n",
      "train loss:0.04268096178675432\n",
      "train loss:0.028619729796784784\n",
      "train loss:0.09041599586098854\n",
      "train loss:0.0839790075422764\n",
      "train loss:0.1749715217122143\n",
      "train loss:0.11152243739560698\n",
      "train loss:0.06269596688372767\n",
      "train loss:0.038055546940499924\n",
      "train loss:0.08381572788622033\n",
      "train loss:0.06121205440521875\n",
      "train loss:0.06401928538737883\n",
      "train loss:0.05361444278214797\n",
      "train loss:0.07690203886518357\n",
      "train loss:0.1360508212575007\n",
      "train loss:0.015302589976748604\n",
      "train loss:0.038626514147835175\n",
      "train loss:0.04774332508633931\n",
      "train loss:0.10076079987968509\n",
      "train loss:0.04231679790245134\n",
      "train loss:0.18939221196986147\n",
      "train loss:0.04863324544763037\n",
      "train loss:0.05796773821875558\n",
      "train loss:0.06328569446275913\n",
      "train loss:0.0694916482341081\n",
      "train loss:0.04268610916845792\n",
      "train loss:0.06584955141734347\n",
      "train loss:0.029620632054849598\n",
      "train loss:0.061109047705282\n",
      "train loss:0.10110857873649541\n",
      "train loss:0.0614685881376599\n",
      "train loss:0.015287418360966564\n",
      "train loss:0.11084075411679054\n",
      "train loss:0.1068891026987975\n",
      "train loss:0.06985398245660622\n",
      "train loss:0.050920257071947604\n",
      "train loss:0.18374223444582796\n",
      "train loss:0.03523826290629837\n",
      "train loss:0.02434032486675145\n",
      "train loss:0.04890208311811863\n",
      "train loss:0.09413462629666507\n",
      "train loss:0.051926852075715776\n",
      "train loss:0.11138948099485979\n",
      "train loss:0.0273883683469157\n",
      "train loss:0.19961839650225396\n",
      "train loss:0.06124213837337593\n",
      "train loss:0.026089621918797788\n",
      "train loss:0.12718102998695816\n",
      "train loss:0.21426157015548664\n",
      "train loss:0.07221667434523464\n",
      "train loss:0.06273665141135758\n",
      "train loss:0.04389457873943215\n",
      "train loss:0.0937869177984794\n",
      "train loss:0.05515827927758645\n",
      "train loss:0.04272584481893472\n",
      "train loss:0.09905249625499098\n",
      "train loss:0.0940033178790223\n",
      "train loss:0.1292713465940493\n",
      "train loss:0.11190566744635078\n",
      "train loss:0.057735959781880154\n",
      "train loss:0.01595945473018044\n",
      "train loss:0.18973795736717539\n",
      "train loss:0.09845983343223981\n",
      "train loss:0.029542824402848167\n",
      "train loss:0.08852189271793169\n",
      "train loss:0.06031476253139132\n",
      "train loss:0.05119070898153932\n",
      "train loss:0.04774624214949681\n",
      "train loss:0.09730493065610892\n",
      "train loss:0.07163223726399492\n",
      "train loss:0.05184976247160413\n",
      "train loss:0.0562046840406491\n",
      "train loss:0.05382434727179277\n",
      "train loss:0.11320588004820362\n",
      "train loss:0.08665416229981467\n",
      "train loss:0.052241493989433695\n",
      "train loss:0.03697340180026334\n",
      "train loss:0.03395034545516566\n",
      "train loss:0.13407099093081706\n",
      "train loss:0.06800570028408547\n",
      "train loss:0.06858175526448686\n",
      "train loss:0.04739580731947823\n",
      "train loss:0.0389583914435319\n",
      "train loss:0.03790756209000806\n",
      "train loss:0.03143376283090907\n",
      "train loss:0.126507338889062\n",
      "train loss:0.070813922199611\n",
      "train loss:0.02727321440663784\n",
      "train loss:0.030811246573812644\n",
      "train loss:0.025525890425967958\n",
      "train loss:0.07774159479580474\n",
      "train loss:0.04379291184741195\n",
      "train loss:0.04581265299358517\n",
      "train loss:0.07940504415152527\n",
      "train loss:0.03470068350852195\n",
      "train loss:0.044743392134899695\n",
      "train loss:0.07766448138971992\n",
      "train loss:0.05308769475978857\n",
      "train loss:0.07397335255108177\n",
      "train loss:0.07195837694805549\n",
      "train loss:0.0945966729324665\n",
      "train loss:0.04184991897776211\n",
      "train loss:0.017981282560070942\n",
      "train loss:0.04407598754949025\n",
      "train loss:0.06359319305086861\n",
      "train loss:0.09027107361796582\n",
      "train loss:0.11794973465613355\n",
      "train loss:0.01810720639015528\n",
      "train loss:0.09398756400403813\n",
      "train loss:0.08890149711615884\n",
      "train loss:0.09529945633211113\n",
      "train loss:0.042206953651635085\n",
      "train loss:0.1538223517725195\n",
      "train loss:0.0931119089456061\n",
      "train loss:0.03553015731850083\n",
      "train loss:0.04441260853243739\n",
      "train loss:0.06929418738366958\n",
      "train loss:0.04318406338065219\n",
      "train loss:0.06337233360670355\n",
      "train loss:0.0847699866042226\n",
      "train loss:0.04785531881208909\n",
      "train loss:0.0853277866736229\n",
      "train loss:0.015051330136158957\n",
      "train loss:0.05003302991839026\n",
      "train loss:0.09765671787575916\n",
      "train loss:0.07406353156568689\n",
      "train loss:0.02112733083865466\n",
      "train loss:0.1059556104170058\n",
      "train loss:0.04240793732867169\n",
      "train loss:0.0882754470994352\n",
      "train loss:0.09154946332973363\n",
      "train loss:0.0947305630466944\n",
      "train loss:0.04346821208473806\n",
      "train loss:0.07345518361937949\n",
      "train loss:0.08352627333973413\n",
      "train loss:0.1362520584976142\n",
      "train loss:0.14257922915035226\n",
      "train loss:0.08133406780120898\n",
      "train loss:0.058041675327839055\n",
      "train loss:0.10906545920968323\n",
      "train loss:0.05800203828282864\n",
      "train loss:0.07499425080736469\n",
      "train loss:0.07014142712540035\n",
      "train loss:0.10152363212492428\n",
      "train loss:0.06677052558285952\n",
      "train loss:0.03829841362801674\n",
      "train loss:0.03820156274624813\n",
      "train loss:0.017821812175365655\n",
      "train loss:0.08179219825133749\n",
      "train loss:0.0657292996211675\n",
      "train loss:0.06301072170643499\n",
      "train loss:0.04512355043632474\n",
      "train loss:0.05888800860749706\n",
      "train loss:0.04035685380574963\n",
      "train loss:0.11910844659069911\n",
      "train loss:0.047818447350929764\n",
      "train loss:0.16146474266690192\n",
      "train loss:0.11681292663187114\n",
      "train loss:0.054878774448260115\n",
      "train loss:0.10698136149949176\n",
      "train loss:0.020907174738367496\n",
      "train loss:0.048934906303805974\n",
      "train loss:0.028318413385828\n",
      "train loss:0.03015337243486257\n",
      "train loss:0.04000948137332581\n",
      "train loss:0.08449812526273914\n",
      "train loss:0.0923447988229442\n",
      "train loss:0.03606299126798528\n",
      "train loss:0.05217397754813596\n",
      "train loss:0.07185082457241883\n",
      "train loss:0.060876072852303356\n",
      "train loss:0.043943347344861376\n",
      "train loss:0.05359874636998365\n",
      "train loss:0.057141448568983746\n",
      "train loss:0.12661056972485865\n",
      "train loss:0.02552381102325068\n",
      "train loss:0.05616361571137239\n",
      "train loss:0.1118726296201652\n",
      "train loss:0.04219276019825103\n",
      "train loss:0.05771175278544229\n",
      "train loss:0.16365065967113046\n",
      "train loss:0.06967809107094684\n",
      "train loss:0.11949951333495554\n",
      "train loss:0.029864681754683806\n",
      "train loss:0.021221418724354044\n",
      "train loss:0.1386069249727927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04298645986671582\n",
      "train loss:0.03652411526304304\n",
      "train loss:0.10688556087894369\n",
      "train loss:0.054305691520720345\n",
      "train loss:0.031984698536782104\n",
      "train loss:0.0715780618484914\n",
      "train loss:0.016474044986891045\n",
      "train loss:0.04519445473402678\n",
      "train loss:0.0267244433939947\n",
      "train loss:0.030145849839506786\n",
      "train loss:0.05703928058451977\n",
      "train loss:0.05454472122617242\n",
      "train loss:0.06531362625236818\n",
      "train loss:0.06341502946251745\n",
      "train loss:0.10899210843656527\n",
      "train loss:0.06976538982368297\n",
      "train loss:0.03930688074852568\n",
      "train loss:0.02816125559836984\n",
      "train loss:0.032193992792121554\n",
      "train loss:0.09955261113144692\n",
      "train loss:0.19307179357123172\n",
      "train loss:0.038148190127012524\n",
      "train loss:0.06738169393166743\n",
      "train loss:0.0407869014978051\n",
      "train loss:0.10934249842171724\n",
      "train loss:0.015543115403701473\n",
      "train loss:0.07400614913443217\n",
      "train loss:0.051260420576393254\n",
      "train loss:0.057938783957342775\n",
      "train loss:0.07771189672799339\n",
      "train loss:0.05869394843070521\n",
      "train loss:0.045513844598061404\n",
      "train loss:0.06135741142875773\n",
      "train loss:0.061929636346572996\n",
      "train loss:0.030212786444507343\n",
      "train loss:0.07163710473338034\n",
      "train loss:0.08336511772785714\n",
      "train loss:0.12787723087029165\n",
      "train loss:0.0794644345155639\n",
      "train loss:0.03139518729076363\n",
      "train loss:0.02931880205879625\n",
      "train loss:0.05463171802232607\n",
      "train loss:0.027626383148276697\n",
      "train loss:0.033329374431735795\n",
      "train loss:0.07527055637886394\n",
      "train loss:0.08650377600369559\n",
      "train loss:0.14891953347535666\n",
      "train loss:0.06875074935921587\n",
      "train loss:0.08775609527999047\n",
      "train loss:0.08086444263605463\n",
      "train loss:0.05012215670522427\n",
      "train loss:0.03818944041894104\n",
      "train loss:0.07019422771463034\n",
      "train loss:0.034192318388895394\n",
      "train loss:0.07980854574691491\n",
      "train loss:0.039495207650558896\n",
      "train loss:0.06064313044840376\n",
      "train loss:0.06061979853540325\n",
      "train loss:0.09030418683704773\n",
      "train loss:0.11969109086825944\n",
      "train loss:0.04617254528129704\n",
      "train loss:0.06941571916621608\n",
      "train loss:0.031062343214107804\n",
      "train loss:0.0725135488496741\n",
      "train loss:0.05341343901622056\n",
      "train loss:0.03139178978162804\n",
      "train loss:0.028601400839156813\n",
      "train loss:0.028598819778679282\n",
      "train loss:0.04629003497111306\n",
      "train loss:0.07866889010947713\n",
      "train loss:0.06625636123724671\n",
      "train loss:0.04267064244477994\n",
      "train loss:0.07480686928094275\n",
      "train loss:0.050948697838651504\n",
      "train loss:0.07394541330555844\n",
      "train loss:0.016027277829280925\n",
      "train loss:0.044187426821304915\n",
      "train loss:0.0737687468017208\n",
      "train loss:0.019976906029304336\n",
      "train loss:0.053011836989138555\n",
      "train loss:0.042914226509177575\n",
      "train loss:0.06064199999466183\n",
      "train loss:0.15293769990470724\n",
      "train loss:0.09156470955120871\n",
      "train loss:0.1072455089447383\n",
      "train loss:0.08655840480222085\n",
      "train loss:0.08759533994158614\n",
      "train loss:0.16871043200197933\n",
      "train loss:0.06118785887228973\n",
      "train loss:0.03456030129447259\n",
      "train loss:0.02500345084568626\n",
      "train loss:0.03551243902399016\n",
      "train loss:0.08310261343031325\n",
      "train loss:0.06223464003613792\n",
      "train loss:0.11361916432153572\n",
      "train loss:0.03536828015922781\n",
      "train loss:0.021409607726477646\n",
      "train loss:0.035654240722182415\n",
      "train loss:0.08816265476614946\n",
      "train loss:0.01775975066312329\n",
      "train loss:0.08061843784097285\n",
      "train loss:0.06242471009261217\n",
      "train loss:0.10376596785386111\n",
      "train loss:0.07218620118238436\n",
      "train loss:0.05065860881400369\n",
      "train loss:0.01926034686273629\n",
      "train loss:0.059250422215817354\n",
      "train loss:0.051676713470652676\n",
      "train loss:0.07961813564262085\n",
      "train loss:0.032382436070217346\n",
      "train loss:0.1143048869638309\n",
      "train loss:0.020109450300476853\n",
      "train loss:0.07913974898792414\n",
      "train loss:0.05091813257392468\n",
      "train loss:0.028456497029621334\n",
      "train loss:0.033826868079373684\n",
      "train loss:0.05046592949140767\n",
      "train loss:0.025190496342838795\n",
      "train loss:0.033799445817052694\n",
      "train loss:0.026970206687510743\n",
      "train loss:0.020340385426088868\n",
      "train loss:0.07051546399654092\n",
      "train loss:0.07933383438643822\n",
      "train loss:0.02954566188821717\n",
      "train loss:0.05512924145871032\n",
      "train loss:0.06988770152032064\n",
      "train loss:0.06875570074954708\n",
      "train loss:0.10541771249198792\n",
      "train loss:0.04457398180192323\n",
      "train loss:0.11925980581576859\n",
      "train loss:0.05850846937344398\n",
      "train loss:0.034588079887662056\n",
      "train loss:0.04185032678054805\n",
      "train loss:0.1487930067102509\n",
      "train loss:0.08837629310697193\n",
      "train loss:0.09290530237711782\n",
      "=== epoch:3, train acc:0.981, test acc:0.978 ===\n",
      "train loss:0.024558767601954506\n",
      "train loss:0.06335023161833236\n",
      "train loss:0.017432979831345178\n",
      "train loss:0.07875899657067528\n",
      "train loss:0.040635955173564386\n",
      "train loss:0.02864079255055558\n",
      "train loss:0.03971126654895096\n",
      "train loss:0.11853893718486554\n",
      "train loss:0.07796264818655847\n",
      "train loss:0.10490639918208051\n",
      "train loss:0.033391538961569454\n",
      "train loss:0.06385971681231256\n",
      "train loss:0.04805975545568059\n",
      "train loss:0.030695790407901664\n",
      "train loss:0.04688031609675296\n",
      "train loss:0.09613384891173571\n",
      "train loss:0.03898920316991268\n",
      "train loss:0.026121546890340595\n",
      "train loss:0.04934842839646917\n",
      "train loss:0.1360459013539705\n",
      "train loss:0.08866298074260313\n",
      "train loss:0.078732812326689\n",
      "train loss:0.04012478063897763\n",
      "train loss:0.052433034653317556\n",
      "train loss:0.07797810137377896\n",
      "train loss:0.090144237779072\n",
      "train loss:0.06690011202597466\n",
      "train loss:0.08073718754388376\n",
      "train loss:0.07531376572569622\n",
      "train loss:0.058724184834509634\n",
      "train loss:0.030839376072498644\n",
      "train loss:0.06322174303827194\n",
      "train loss:0.06734173302046026\n",
      "train loss:0.018529243294220343\n",
      "train loss:0.027604316627681987\n",
      "train loss:0.046274777241234716\n",
      "train loss:0.07337638630882563\n",
      "train loss:0.02125021495888574\n",
      "train loss:0.05556140135371862\n",
      "train loss:0.06291933732429142\n",
      "train loss:0.014792296305169983\n",
      "train loss:0.059380144821198494\n",
      "train loss:0.08670795280580419\n",
      "train loss:0.043006629969850316\n",
      "train loss:0.049511676537823096\n",
      "train loss:0.03461801879706575\n",
      "train loss:0.03282625459302049\n",
      "train loss:0.10484460661432114\n",
      "train loss:0.0957515133694746\n",
      "train loss:0.040308994321481356\n",
      "train loss:0.06424132728233795\n",
      "train loss:0.07997023473625568\n",
      "train loss:0.07984492362356313\n",
      "train loss:0.07267827061408755\n",
      "train loss:0.04211215897032906\n",
      "train loss:0.08820396980182173\n",
      "train loss:0.036570710400561854\n",
      "train loss:0.03807631121019275\n",
      "train loss:0.03489540862788647\n",
      "train loss:0.08051876176114252\n",
      "train loss:0.05483513421138801\n",
      "train loss:0.04893028463153276\n",
      "train loss:0.08003449504563674\n",
      "train loss:0.06416503936535711\n",
      "train loss:0.013860966134931684\n",
      "train loss:0.015771838889606295\n",
      "train loss:0.09329480523350442\n",
      "train loss:0.05764378917639438\n",
      "train loss:0.05854901882481006\n",
      "train loss:0.042458888399403615\n",
      "train loss:0.10620389733046176\n",
      "train loss:0.06368071508212632\n",
      "train loss:0.06601967156405407\n",
      "train loss:0.028219589826927884\n",
      "train loss:0.0521272858834453\n",
      "train loss:0.06083709272770288\n",
      "train loss:0.05615646513618389\n",
      "train loss:0.1431694494483817\n",
      "train loss:0.029075821188809917\n",
      "train loss:0.027246323631492535\n",
      "train loss:0.06068362207276615\n",
      "train loss:0.05272770281502538\n",
      "train loss:0.022138203011226737\n",
      "train loss:0.05642422717364275\n",
      "train loss:0.05812905548033723\n",
      "train loss:0.07678380638909453\n",
      "train loss:0.03489208896450818\n",
      "train loss:0.034401328915842425\n",
      "train loss:0.04509150902009968\n",
      "train loss:0.11303018454379161\n",
      "train loss:0.032235312298105334\n",
      "train loss:0.05993190767020638\n",
      "train loss:0.02271790713293474\n",
      "train loss:0.028647060814724134\n",
      "train loss:0.05741646012436816\n",
      "train loss:0.06535277051045969\n",
      "train loss:0.027161740732704043\n",
      "train loss:0.04161180660008267\n",
      "train loss:0.0665132887441225\n",
      "train loss:0.03332387956092565\n",
      "train loss:0.03970097722091795\n",
      "train loss:0.15892363406448914\n",
      "train loss:0.05423139099397049\n",
      "train loss:0.03928293276971648\n",
      "train loss:0.03874492390944662\n",
      "train loss:0.055483578468729255\n",
      "train loss:0.023997332978959732\n",
      "train loss:0.035278669207325475\n",
      "train loss:0.023104983330201546\n",
      "train loss:0.02855040674967664\n",
      "train loss:0.026720084901485187\n",
      "train loss:0.10539949608713473\n",
      "train loss:0.027414200851045557\n",
      "train loss:0.021443274210836957\n",
      "train loss:0.05442947879225212\n",
      "train loss:0.01937471969967395\n",
      "train loss:0.07978531193715566\n",
      "train loss:0.03251765313435905\n",
      "train loss:0.04096413951200172\n",
      "train loss:0.09907584097771746\n",
      "train loss:0.015647251653401473\n",
      "train loss:0.021431455240481713\n",
      "train loss:0.0179165301745824\n",
      "train loss:0.0523901342656331\n",
      "train loss:0.07069922190756418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01892199535469645\n",
      "train loss:0.048513355625723974\n",
      "train loss:0.03239036792149629\n",
      "train loss:0.03503591838301086\n",
      "train loss:0.053483528988163845\n",
      "train loss:0.02853449050193909\n",
      "train loss:0.08642974163481311\n",
      "train loss:0.02327295150285498\n",
      "train loss:0.05339564425465029\n",
      "train loss:0.0468221049796127\n",
      "train loss:0.11224399155095066\n",
      "train loss:0.0671194241848982\n",
      "train loss:0.05446073141922605\n",
      "train loss:0.025944266531239556\n",
      "train loss:0.04346800199114717\n",
      "train loss:0.07227018050193944\n",
      "train loss:0.03993023968239271\n",
      "train loss:0.13623784084963558\n",
      "train loss:0.04408156352536622\n",
      "train loss:0.09451268512355758\n",
      "train loss:0.014621206539778322\n",
      "train loss:0.179726751502419\n",
      "train loss:0.030755419646501303\n",
      "train loss:0.029320434158354204\n",
      "train loss:0.018400458336450713\n",
      "train loss:0.07840905547960014\n",
      "train loss:0.020970433480279303\n",
      "train loss:0.05581544469072115\n",
      "train loss:0.026164608478763864\n",
      "train loss:0.039599050515268804\n",
      "train loss:0.05793087372495993\n",
      "train loss:0.025985613155801675\n",
      "train loss:0.06149144389819021\n",
      "train loss:0.09547606874762543\n",
      "train loss:0.09334740225021802\n",
      "train loss:0.018549475465039697\n",
      "train loss:0.03745155527103257\n",
      "train loss:0.02712375227207573\n",
      "train loss:0.013138907125923117\n",
      "train loss:0.0678668925470185\n",
      "train loss:0.029660561959430214\n",
      "train loss:0.028701392151852616\n",
      "train loss:0.05899770501417814\n",
      "train loss:0.027864164397788035\n",
      "train loss:0.025599168560544033\n",
      "train loss:0.06597695958568957\n",
      "train loss:0.07427854164714925\n",
      "train loss:0.03317456246805652\n",
      "train loss:0.019605033127963432\n",
      "train loss:0.026651719845570684\n",
      "train loss:0.06724666875436536\n",
      "train loss:0.041961761339892496\n",
      "train loss:0.03746115389304049\n",
      "train loss:0.07877350635421629\n",
      "train loss:0.0370618877104652\n",
      "train loss:0.0998137757609125\n",
      "train loss:0.056862362137174535\n",
      "train loss:0.04177703984528535\n",
      "train loss:0.03126855445198671\n",
      "train loss:0.061881333679248696\n",
      "train loss:0.03986756807611641\n",
      "train loss:0.0714858492811671\n",
      "train loss:0.10464470380172136\n",
      "train loss:0.03234872897855563\n",
      "train loss:0.046858973798435184\n",
      "train loss:0.016279908449445955\n",
      "train loss:0.02591591136990958\n",
      "train loss:0.05719850971109459\n",
      "train loss:0.04891843365652021\n",
      "train loss:0.030014222200559472\n",
      "train loss:0.09190436591005252\n",
      "train loss:0.03060915797713461\n",
      "train loss:0.11970929156731452\n",
      "train loss:0.059891939150574454\n",
      "train loss:0.047154235542227724\n",
      "train loss:0.03823044430947533\n",
      "train loss:0.0252726664291933\n",
      "train loss:0.14977869732700502\n",
      "train loss:0.04068149988005467\n",
      "train loss:0.033975374635219456\n",
      "train loss:0.047197530240346994\n",
      "train loss:0.0117339233216709\n",
      "train loss:0.0953591269581968\n",
      "train loss:0.07099868588096567\n",
      "train loss:0.01785551024729977\n",
      "train loss:0.042349372718190274\n",
      "train loss:0.0626291481693268\n",
      "train loss:0.0341816801182472\n",
      "train loss:0.039291663682459156\n",
      "train loss:0.08098149328329747\n",
      "train loss:0.06888023965795241\n",
      "train loss:0.008827522870922463\n",
      "train loss:0.21384434667904306\n",
      "train loss:0.047292113790095264\n",
      "train loss:0.09817018088719585\n",
      "train loss:0.03807892155682514\n",
      "train loss:0.014156069524299415\n",
      "train loss:0.05260043014344722\n",
      "train loss:0.05961272161341963\n",
      "train loss:0.05277134673690262\n",
      "train loss:0.05202394426318411\n",
      "train loss:0.0785904738655669\n",
      "train loss:0.08980916991260292\n",
      "train loss:0.037968520287139304\n",
      "train loss:0.045484132022403256\n",
      "train loss:0.027185102290955346\n",
      "train loss:0.02484600523028372\n",
      "train loss:0.05414819033763693\n",
      "train loss:0.033877331516468635\n",
      "train loss:0.021962944561476724\n",
      "train loss:0.08084052176455261\n",
      "train loss:0.0334579675690307\n",
      "train loss:0.15099737060833585\n",
      "train loss:0.07338800350644697\n",
      "train loss:0.05078637668256243\n",
      "train loss:0.03679407080601675\n",
      "train loss:0.08172915228092928\n",
      "train loss:0.04443781407759292\n",
      "train loss:0.02878617634506372\n",
      "train loss:0.16567373409938843\n",
      "train loss:0.03054927317444918\n",
      "train loss:0.04479513008346115\n",
      "train loss:0.14291556476584993\n",
      "train loss:0.10668428929748464\n",
      "train loss:0.07196875391969676\n",
      "train loss:0.014670536983965567\n",
      "train loss:0.020895606253174614\n",
      "train loss:0.03348001137425339\n",
      "train loss:0.032363216218382965\n",
      "train loss:0.0640594383515632\n",
      "train loss:0.021502485467666786\n",
      "train loss:0.021440458084265594\n",
      "train loss:0.11219635773325519\n",
      "train loss:0.05140048009982324\n",
      "train loss:0.020838749902705237\n",
      "train loss:0.03510320951025484\n",
      "train loss:0.05135554206429727\n",
      "train loss:0.15907292149200425\n",
      "train loss:0.11127948583064344\n",
      "train loss:0.030778451004876892\n",
      "train loss:0.09051642389973454\n",
      "train loss:0.044164127013308754\n",
      "train loss:0.052857714178541355\n",
      "train loss:0.04981853984430128\n",
      "train loss:0.10458102359599863\n",
      "train loss:0.09161939024536846\n",
      "train loss:0.09311461900149746\n",
      "train loss:0.05522069406834051\n",
      "train loss:0.035323775030317923\n",
      "train loss:0.034119114134574746\n",
      "train loss:0.10497478116199738\n",
      "train loss:0.09854610947392098\n",
      "train loss:0.06600335767464985\n",
      "train loss:0.01837416680540649\n",
      "train loss:0.023162115943526258\n",
      "train loss:0.01852971319390229\n",
      "train loss:0.0625277772899499\n",
      "train loss:0.054758868252639294\n",
      "train loss:0.036485425998114725\n",
      "train loss:0.023669604552033163\n",
      "train loss:0.035238333638072265\n",
      "train loss:0.02124659650536216\n",
      "train loss:0.04160886289256366\n",
      "train loss:0.06666102593702035\n",
      "train loss:0.11027550454030477\n",
      "train loss:0.043244140320258806\n",
      "train loss:0.01533755583462136\n",
      "train loss:0.03507917810346629\n",
      "train loss:0.055903800012384944\n",
      "train loss:0.03992526648371024\n",
      "train loss:0.033165630066596855\n",
      "train loss:0.054903958890645976\n",
      "train loss:0.016921878581253234\n",
      "train loss:0.013974088458248499\n",
      "train loss:0.04043564629005603\n",
      "train loss:0.008373865091990168\n",
      "train loss:0.09528381763000292\n",
      "train loss:0.06886556888752285\n",
      "train loss:0.10679786005188203\n",
      "train loss:0.036160982770758264\n",
      "train loss:0.07576495692485213\n",
      "train loss:0.06474933366884372\n",
      "train loss:0.012240929026315035\n",
      "train loss:0.11720679451108613\n",
      "train loss:0.014063120811920246\n",
      "train loss:0.049098246393181146\n",
      "train loss:0.051917314149207465\n",
      "train loss:0.021769573832971317\n",
      "train loss:0.04346820212566558\n",
      "train loss:0.02500551827527747\n",
      "train loss:0.04792838356431047\n",
      "train loss:0.04577302782424249\n",
      "train loss:0.06147517571576303\n",
      "train loss:0.1079540853469143\n",
      "train loss:0.054490583934769264\n",
      "train loss:0.05782962833529411\n",
      "train loss:0.029928664053181783\n",
      "train loss:0.09832429936602789\n",
      "train loss:0.05895566464300475\n",
      "train loss:0.023295754231158153\n",
      "train loss:0.0872250292691288\n",
      "train loss:0.047490732014611584\n",
      "train loss:0.027812014809425616\n",
      "train loss:0.015261504689998562\n",
      "train loss:0.07394565963213666\n",
      "train loss:0.017218475899993303\n",
      "train loss:0.03892420112174436\n",
      "train loss:0.14093432083047297\n",
      "train loss:0.05382649302217444\n",
      "train loss:0.05577536308979293\n",
      "train loss:0.04122221476993379\n",
      "train loss:0.11585393226088589\n",
      "train loss:0.022958247790700232\n",
      "train loss:0.023661960078178966\n",
      "train loss:0.009080481737308934\n",
      "train loss:0.05673170775376762\n",
      "train loss:0.047145672778616615\n",
      "train loss:0.01807012459561428\n",
      "train loss:0.06321145710316425\n",
      "train loss:0.04260839270151611\n",
      "train loss:0.013282716084464201\n",
      "train loss:0.08309711272551581\n",
      "train loss:0.08246816930146253\n",
      "train loss:0.10401117290203186\n",
      "train loss:0.030665866084670125\n",
      "train loss:0.03519727329668761\n",
      "train loss:0.06408119955217446\n",
      "train loss:0.04326039341206033\n",
      "train loss:0.030202392054978048\n",
      "train loss:0.05385371775341812\n",
      "train loss:0.07297505949579913\n",
      "train loss:0.0732529886410582\n",
      "train loss:0.06469817411040005\n",
      "train loss:0.15049920243816053\n",
      "train loss:0.03541707597595596\n",
      "train loss:0.02714505937219896\n",
      "train loss:0.06048572778879285\n",
      "train loss:0.037603289537255995\n",
      "train loss:0.09006811337386429\n",
      "train loss:0.032418465252727874\n",
      "train loss:0.025454114119167495\n",
      "train loss:0.03086195857074498\n",
      "train loss:0.05858768544329905\n",
      "train loss:0.04098857751491297\n",
      "train loss:0.06197429430997731\n",
      "train loss:0.028692157506883656\n",
      "train loss:0.07990854082351312\n",
      "train loss:0.14933907369324376\n",
      "train loss:0.03702315116179799\n",
      "train loss:0.03383235666155864\n",
      "train loss:0.04439459990515043\n",
      "train loss:0.09747650074301094\n",
      "train loss:0.01250144706648475\n",
      "train loss:0.03326658063285697\n",
      "train loss:0.023429536923256854\n",
      "train loss:0.08086171879263626\n",
      "train loss:0.04488886319380722\n",
      "train loss:0.026603381271962136\n",
      "train loss:0.05731839966563439\n",
      "train loss:0.01271097340983409\n",
      "train loss:0.01233087958946289\n",
      "train loss:0.03683782321584392\n",
      "train loss:0.018788982013947988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.053491480342083585\n",
      "train loss:0.058140789091469115\n",
      "train loss:0.025550258327414373\n",
      "train loss:0.057394896148817647\n",
      "train loss:0.012836706703039491\n",
      "train loss:0.13351164112028413\n",
      "train loss:0.11930849949401397\n",
      "train loss:0.06867253628976534\n",
      "train loss:0.019465879192180523\n",
      "train loss:0.10757093979696288\n",
      "train loss:0.027737158449376534\n",
      "train loss:0.022081904319874012\n",
      "train loss:0.03753674316942583\n",
      "train loss:0.01909875913971834\n",
      "train loss:0.023017197687680974\n",
      "train loss:0.08485074888908574\n",
      "train loss:0.027234588208218495\n",
      "train loss:0.05885029664421183\n",
      "train loss:0.05255457066397635\n",
      "train loss:0.04258822019680344\n",
      "train loss:0.008807208051298258\n",
      "train loss:0.015637721327640592\n",
      "train loss:0.03861143814869378\n",
      "train loss:0.030616276025245838\n",
      "train loss:0.05082379812316726\n",
      "train loss:0.030249347854862666\n",
      "train loss:0.031247682322390308\n",
      "train loss:0.0354638185474206\n",
      "train loss:0.03756975736438035\n",
      "train loss:0.023712839342929993\n",
      "train loss:0.06364029292344349\n",
      "train loss:0.007177862944096722\n",
      "train loss:0.09915883530608655\n",
      "train loss:0.023775150005602343\n",
      "train loss:0.13226652599761898\n",
      "train loss:0.06879188429392884\n",
      "train loss:0.012764564983325446\n",
      "train loss:0.03971130468618536\n",
      "train loss:0.011905973435102381\n",
      "train loss:0.05523519418287525\n",
      "train loss:0.009176601680998202\n",
      "train loss:0.12013372833772178\n",
      "train loss:0.10925467025411714\n",
      "train loss:0.02255657775558888\n",
      "train loss:0.029616652270736735\n",
      "train loss:0.008216289676826638\n",
      "train loss:0.058475890219111176\n",
      "train loss:0.025017468311074636\n",
      "train loss:0.016639097808478283\n",
      "train loss:0.017008635211350834\n",
      "train loss:0.012106934941784173\n",
      "train loss:0.029802457120733847\n",
      "train loss:0.04607341096574251\n",
      "train loss:0.014277263838668373\n",
      "train loss:0.0238138870832476\n",
      "train loss:0.015295693944036735\n",
      "train loss:0.014855877820185144\n",
      "train loss:0.02436530959882299\n",
      "train loss:0.0721047308651791\n",
      "train loss:0.0161532528560852\n",
      "train loss:0.15641105010093845\n",
      "train loss:0.027708312188043212\n",
      "train loss:0.01565315028860776\n",
      "train loss:0.016067971032212434\n",
      "train loss:0.03667076845632061\n",
      "train loss:0.09035825717390829\n",
      "train loss:0.016987241291764658\n",
      "train loss:0.03722275042609832\n",
      "train loss:0.03332438905908515\n",
      "train loss:0.039336277500987234\n",
      "train loss:0.019145400128525687\n",
      "train loss:0.02224625210232269\n",
      "train loss:0.036734642696188674\n",
      "train loss:0.016353460909408767\n",
      "train loss:0.01654447674665317\n",
      "train loss:0.052153602058838254\n",
      "train loss:0.03796502446943125\n",
      "train loss:0.022477007750307972\n",
      "train loss:0.05795239976172597\n",
      "train loss:0.016277321179828302\n",
      "train loss:0.05450862112235987\n",
      "train loss:0.021407956222995392\n",
      "train loss:0.025157268447010807\n",
      "train loss:0.08817537946347001\n",
      "train loss:0.047370185333643545\n",
      "train loss:0.018483606654687786\n",
      "train loss:0.07258498796503104\n",
      "train loss:0.014842855333998185\n",
      "train loss:0.037391390461782914\n",
      "train loss:0.00875232408119404\n",
      "train loss:0.03742067147540839\n",
      "train loss:0.1353155297044278\n",
      "train loss:0.0244688729558798\n",
      "train loss:0.11165979993605543\n",
      "train loss:0.029228554138756656\n",
      "train loss:0.03632556546561731\n",
      "train loss:0.052435698775643684\n",
      "train loss:0.04965055210821772\n",
      "train loss:0.03788491647897612\n",
      "train loss:0.056535812770256495\n",
      "train loss:0.047794198539675786\n",
      "train loss:0.07801506642969218\n",
      "train loss:0.03587886109306053\n",
      "train loss:0.016810617478074753\n",
      "train loss:0.017762385209232175\n",
      "train loss:0.06111132620621206\n",
      "train loss:0.07873532764017102\n",
      "train loss:0.1155065411359615\n",
      "train loss:0.10372111404902647\n",
      "train loss:0.03920592779923454\n",
      "train loss:0.03560001679739932\n",
      "train loss:0.022737469317684098\n",
      "train loss:0.031238289989601778\n",
      "train loss:0.05148000165134379\n",
      "train loss:0.05911727977905515\n",
      "train loss:0.03932762304848145\n",
      "train loss:0.011957130530296501\n",
      "train loss:0.056850419540578054\n",
      "train loss:0.023185484189864963\n",
      "train loss:0.07151471799477621\n",
      "train loss:0.051555507023465064\n",
      "train loss:0.06230453332706582\n",
      "train loss:0.03789534379340232\n",
      "train loss:0.021623798917299215\n",
      "train loss:0.02382019564458038\n",
      "train loss:0.010179398255236602\n",
      "train loss:0.03896910977502\n",
      "train loss:0.03313425203844482\n",
      "train loss:0.028201888123271693\n",
      "train loss:0.07847113655708675\n",
      "train loss:0.03743968579319428\n",
      "train loss:0.02116696099655008\n",
      "train loss:0.040150384523569914\n",
      "train loss:0.024029740719403628\n",
      "train loss:0.07183973939362803\n",
      "train loss:0.010406270960311692\n",
      "train loss:0.0237563267824248\n",
      "train loss:0.033763066519807136\n",
      "train loss:0.06287994449090335\n",
      "train loss:0.049234225833930195\n",
      "train loss:0.04048941180973932\n",
      "train loss:0.09034801444229038\n",
      "train loss:0.1816569195682483\n",
      "train loss:0.12011289592756652\n",
      "train loss:0.07537529763605269\n",
      "train loss:0.07765542323599817\n",
      "train loss:0.13340539486581243\n",
      "train loss:0.08622523375064446\n",
      "train loss:0.010935937264833663\n",
      "train loss:0.07564102029468392\n",
      "train loss:0.03286726363841699\n",
      "train loss:0.021790340168401822\n",
      "train loss:0.06354756636956838\n",
      "train loss:0.038132400937300234\n",
      "train loss:0.0921124104019511\n",
      "train loss:0.02108721885787227\n",
      "train loss:0.035247855383908254\n",
      "train loss:0.06314650649750436\n",
      "train loss:0.0663491944084549\n",
      "train loss:0.12982513201450965\n",
      "train loss:0.018606894424613715\n",
      "train loss:0.046221787797895146\n",
      "train loss:0.08491080899429297\n",
      "train loss:0.032456077813560885\n",
      "train loss:0.07604863115828053\n",
      "train loss:0.051635813393712454\n",
      "train loss:0.01167704081893838\n",
      "train loss:0.051941079034609844\n",
      "train loss:0.06656768040592981\n",
      "train loss:0.01460524606655116\n",
      "train loss:0.02444652735247133\n",
      "train loss:0.02543926227054143\n",
      "train loss:0.04960188649221551\n",
      "train loss:0.05076114743580869\n",
      "train loss:0.03768253426232287\n",
      "train loss:0.0424463975362612\n",
      "train loss:0.03098872670207613\n",
      "train loss:0.016533516866635588\n",
      "train loss:0.04397312161170713\n",
      "train loss:0.055196549530740435\n",
      "train loss:0.042534621018403815\n",
      "train loss:0.06715955345875552\n",
      "train loss:0.0200681177293613\n",
      "train loss:0.016062583976686293\n",
      "train loss:0.012624714540147814\n",
      "train loss:0.018060691284733244\n",
      "train loss:0.011344435667813896\n",
      "train loss:0.04426115402583717\n",
      "train loss:0.018420795987764395\n",
      "train loss:0.016094670204701792\n",
      "train loss:0.03441221979322997\n",
      "train loss:0.030991614298587638\n",
      "train loss:0.0973085039225199\n",
      "train loss:0.04835070247179214\n",
      "train loss:0.10326447160681482\n",
      "train loss:0.03610680266531327\n",
      "train loss:0.053346168597029536\n",
      "train loss:0.08164371708629815\n",
      "train loss:0.017090726767157848\n",
      "train loss:0.027694111048317187\n",
      "train loss:0.10675398042050371\n",
      "train loss:0.14828043639297284\n",
      "train loss:0.0398666942282727\n",
      "train loss:0.07400707239779435\n",
      "train loss:0.04233750793548552\n",
      "train loss:0.02123405007771652\n",
      "train loss:0.06943444984912879\n",
      "train loss:0.03275952604997752\n",
      "train loss:0.04890231399193239\n",
      "train loss:0.028002671601560097\n",
      "train loss:0.02675792486310673\n",
      "train loss:0.05694756012937952\n",
      "=== epoch:4, train acc:0.98, test acc:0.982 ===\n",
      "train loss:0.0332378550590392\n",
      "train loss:0.10345502185288975\n",
      "train loss:0.02559848990795766\n",
      "train loss:0.012986419789127433\n",
      "train loss:0.06362677433019902\n",
      "train loss:0.034467934703923536\n",
      "train loss:0.04354383708626952\n",
      "train loss:0.035452772772092155\n",
      "train loss:0.07505545992561792\n",
      "train loss:0.015309653571203325\n",
      "train loss:0.01712098396302197\n",
      "train loss:0.03661425335765344\n",
      "train loss:0.03545519593163141\n",
      "train loss:0.017049556219138114\n",
      "train loss:0.05708920832734472\n",
      "train loss:0.06417189520822342\n",
      "train loss:0.055477554868821916\n",
      "train loss:0.03050114638723584\n",
      "train loss:0.03921890851686796\n",
      "train loss:0.017079304369099404\n",
      "train loss:0.022658397400029003\n",
      "train loss:0.027274730132285204\n",
      "train loss:0.02611230306494923\n",
      "train loss:0.0325854990114517\n",
      "train loss:0.05241690395814817\n",
      "train loss:0.10095569213841996\n",
      "train loss:0.06807317287749161\n",
      "train loss:0.05301965198594902\n",
      "train loss:0.03693750312583687\n",
      "train loss:0.017961951368681634\n",
      "train loss:0.05552948292832802\n",
      "train loss:0.02890921364061561\n",
      "train loss:0.01091427036227841\n",
      "train loss:0.013583448509430591\n",
      "train loss:0.05013722426163532\n",
      "train loss:0.04541311582345863\n",
      "train loss:0.0404867697889083\n",
      "train loss:0.020843629413086963\n",
      "train loss:0.05114796480383716\n",
      "train loss:0.0274969645498402\n",
      "train loss:0.01668036380025773\n",
      "train loss:0.0129886039808226\n",
      "train loss:0.07452257885708508\n",
      "train loss:0.017457582786407452\n",
      "train loss:0.07561457749440273\n",
      "train loss:0.07275756177070258\n",
      "train loss:0.018785744515856767\n",
      "train loss:0.021157257948249054\n",
      "train loss:0.02734543644092351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.028882208680972386\n",
      "train loss:0.018653847629672576\n",
      "train loss:0.062427428723633396\n",
      "train loss:0.018812917831844723\n",
      "train loss:0.06203491253967673\n",
      "train loss:0.023922086328333623\n",
      "train loss:0.01296394725164264\n",
      "train loss:0.02687361036190908\n",
      "train loss:0.02228920810048889\n",
      "train loss:0.009451440684093343\n",
      "train loss:0.07307853195356047\n",
      "train loss:0.026264416699409713\n",
      "train loss:0.07948602868563\n",
      "train loss:0.06138738199991537\n",
      "train loss:0.06890145520327744\n",
      "train loss:0.02910605890883762\n",
      "train loss:0.03442119472809351\n",
      "train loss:0.025994748031343794\n",
      "train loss:0.006530257844337074\n",
      "train loss:0.015427783718107277\n",
      "train loss:0.02220630745606302\n",
      "train loss:0.06050998081969474\n",
      "train loss:0.06251821136395795\n",
      "train loss:0.039939627417763475\n",
      "train loss:0.027324187336058944\n",
      "train loss:0.028205063158291854\n",
      "train loss:0.06424370065518564\n",
      "train loss:0.022297609043859534\n",
      "train loss:0.0326877434225851\n",
      "train loss:0.047609297006596635\n",
      "train loss:0.08458383922701841\n",
      "train loss:0.05071540928026229\n",
      "train loss:0.06405626226418716\n",
      "train loss:0.012759182440616806\n",
      "train loss:0.01913806775717689\n",
      "train loss:0.05763623311571445\n",
      "train loss:0.01700008304473357\n",
      "train loss:0.03344837211191924\n",
      "train loss:0.03940487644727826\n",
      "train loss:0.05037461396326263\n",
      "train loss:0.04103712961141483\n",
      "train loss:0.010483142130536442\n",
      "train loss:0.06414899123932533\n",
      "train loss:0.047329858768902174\n",
      "train loss:0.02090592275857945\n",
      "train loss:0.04427070081803802\n",
      "train loss:0.03216615160934584\n",
      "train loss:0.006231464687356777\n",
      "train loss:0.021016016859879617\n",
      "train loss:0.010660555115299685\n",
      "train loss:0.040235257309303316\n",
      "train loss:0.02201188092056394\n",
      "train loss:0.06554708483753392\n",
      "train loss:0.008428998292338136\n",
      "train loss:0.03412593032627214\n",
      "train loss:0.06716650994812959\n",
      "train loss:0.030645032358672945\n",
      "train loss:0.04304697936102963\n",
      "train loss:0.03533992858005746\n",
      "train loss:0.0090872260691291\n",
      "train loss:0.01877547588522253\n",
      "train loss:0.007358103159520287\n",
      "train loss:0.022675375231839125\n",
      "train loss:0.042986359085723694\n",
      "train loss:0.023272317205719283\n",
      "train loss:0.037289676201440004\n",
      "train loss:0.018648662816665842\n",
      "train loss:0.0675935142316577\n",
      "train loss:0.010043449383221168\n",
      "train loss:0.021212998238820755\n",
      "train loss:0.014722197681367913\n",
      "train loss:0.024237490901033828\n",
      "train loss:0.1408087598071654\n",
      "train loss:0.03545200494620829\n",
      "train loss:0.02499595255117224\n",
      "train loss:0.02384689008010604\n",
      "train loss:0.040494161678056854\n",
      "train loss:0.06704654968630415\n",
      "train loss:0.025012437465337652\n",
      "train loss:0.01504582314252521\n",
      "train loss:0.04139812733586286\n",
      "train loss:0.07761960889107562\n",
      "train loss:0.014182353118196673\n",
      "train loss:0.024848818210399696\n",
      "train loss:0.05281675946053684\n",
      "train loss:0.060234901039401344\n",
      "train loss:0.016154239001981315\n",
      "train loss:0.042066470220378537\n",
      "train loss:0.036561762138158455\n",
      "train loss:0.015489074319902419\n",
      "train loss:0.044562602211511475\n",
      "train loss:0.02538355124697724\n",
      "train loss:0.04658241632796997\n",
      "train loss:0.09113053204863071\n",
      "train loss:0.03692837820406094\n",
      "train loss:0.06544286285878567\n",
      "train loss:0.029677480617827347\n",
      "train loss:0.022592411149518238\n",
      "train loss:0.02981133990097673\n",
      "train loss:0.0181072504277409\n",
      "train loss:0.03359362900904782\n",
      "train loss:0.055090035314192744\n",
      "train loss:0.08092380580691257\n",
      "train loss:0.042739586585499936\n",
      "train loss:0.009186143502144736\n",
      "train loss:0.011124116701404422\n",
      "train loss:0.04024573338289738\n",
      "train loss:0.01138025310021352\n",
      "train loss:0.019838282337564645\n",
      "train loss:0.1152470614393148\n",
      "train loss:0.026978816979067465\n",
      "train loss:0.010454882069325443\n",
      "train loss:0.025552584615258826\n",
      "train loss:0.057222300005154804\n",
      "train loss:0.07097099596504204\n",
      "train loss:0.029195636170974888\n",
      "train loss:0.021243066532039546\n",
      "train loss:0.0057940755130820665\n",
      "train loss:0.07818306854248455\n",
      "train loss:0.027261690041502626\n",
      "train loss:0.15111380132722144\n",
      "train loss:0.024177392710845572\n",
      "train loss:0.054561206255560456\n",
      "train loss:0.011693863584087742\n",
      "train loss:0.05707194048112136\n",
      "train loss:0.11573445364848008\n",
      "train loss:0.008890105072158374\n",
      "train loss:0.03765485879433922\n",
      "train loss:0.026928227408749245\n",
      "train loss:0.04804490925125382\n",
      "train loss:0.08740738303949025\n",
      "train loss:0.01857190087577092\n",
      "train loss:0.023442548732964686\n",
      "train loss:0.05597934409562175\n",
      "train loss:0.03969618199248958\n",
      "train loss:0.014727339666359814\n",
      "train loss:0.06131071801692906\n",
      "train loss:0.015091244077761311\n",
      "train loss:0.03581747793180051\n",
      "train loss:0.038573310158996815\n",
      "train loss:0.013216158384991565\n",
      "train loss:0.01381922775697035\n",
      "train loss:0.022000127840324773\n",
      "train loss:0.01953598212963947\n",
      "train loss:0.052682490684520876\n",
      "train loss:0.025966749132058892\n",
      "train loss:0.034790911268863293\n",
      "train loss:0.14274962707596883\n",
      "train loss:0.013650951892226103\n",
      "train loss:0.024416706992856044\n",
      "train loss:0.08717591343309614\n",
      "train loss:0.00939479866032129\n",
      "train loss:0.04835520506661497\n",
      "train loss:0.00985202325686488\n",
      "train loss:0.01869667957915398\n",
      "train loss:0.016587589783844274\n",
      "train loss:0.04288608723201282\n",
      "train loss:0.008937190990284283\n",
      "train loss:0.1929576277464112\n",
      "train loss:0.07019125265440666\n",
      "train loss:0.03296751142147749\n",
      "train loss:0.02381796756827465\n",
      "train loss:0.03135776744866483\n",
      "train loss:0.0379633601911344\n",
      "train loss:0.017948442999815843\n",
      "train loss:0.028660717107352317\n",
      "train loss:0.016816178177445514\n",
      "train loss:0.07561636592409253\n",
      "train loss:0.012809346109399102\n",
      "train loss:0.03827460138812725\n",
      "train loss:0.03742086237326486\n",
      "train loss:0.021940753286615225\n",
      "train loss:0.010083313079592897\n",
      "train loss:0.01025832337330246\n",
      "train loss:0.05752950660686176\n",
      "train loss:0.037704057843343894\n",
      "train loss:0.012193338583316999\n",
      "train loss:0.029451756093648567\n",
      "train loss:0.04162730039497793\n",
      "train loss:0.05480561625652352\n",
      "train loss:0.005286011945084762\n",
      "train loss:0.04821368576016525\n",
      "train loss:0.019436807444067995\n",
      "train loss:0.023537150040410827\n",
      "train loss:0.10304281325658422\n",
      "train loss:0.009152226674669606\n",
      "train loss:0.011689876587547048\n",
      "train loss:0.11906888260904987\n",
      "train loss:0.05188305837871134\n",
      "train loss:0.05723228799229609\n",
      "train loss:0.04233030031307734\n",
      "train loss:0.02371481448697596\n",
      "train loss:0.011996693748634156\n",
      "train loss:0.020932261654276182\n",
      "train loss:0.07835103062302956\n",
      "train loss:0.04097940185198635\n",
      "train loss:0.03257236328196901\n",
      "train loss:0.06625575911238574\n",
      "train loss:0.03531985742390165\n",
      "train loss:0.01690163997521859\n",
      "train loss:0.07959663830327529\n",
      "train loss:0.10453851488233477\n",
      "train loss:0.282308651794023\n",
      "train loss:0.008754319748084471\n",
      "train loss:0.23377536276124455\n",
      "train loss:0.009008858737868363\n",
      "train loss:0.016063747356998895\n",
      "train loss:0.04835682660259553\n",
      "train loss:0.01968977455146304\n",
      "train loss:0.04814999452892967\n",
      "train loss:0.11657792527592235\n",
      "train loss:0.07524209354191315\n",
      "train loss:0.08600787153574237\n",
      "train loss:0.01837282191206617\n",
      "train loss:0.04674290042785879\n",
      "train loss:0.09291305922390468\n",
      "train loss:0.03856537931528545\n",
      "train loss:0.019264431410758044\n",
      "train loss:0.028974007036782882\n",
      "train loss:0.04984936795355793\n",
      "train loss:0.040745435082325664\n",
      "train loss:0.027074441037220537\n",
      "train loss:0.0367560932102247\n",
      "train loss:0.015135859618709346\n",
      "train loss:0.03751766465409792\n",
      "train loss:0.05489501658395093\n",
      "train loss:0.006424568619169851\n",
      "train loss:0.020686610756847593\n",
      "train loss:0.04994002381890037\n",
      "train loss:0.035305591967763145\n",
      "train loss:0.03114193866533187\n",
      "train loss:0.03389271154282342\n",
      "train loss:0.08200927932979114\n",
      "train loss:0.04744256407059189\n",
      "train loss:0.0280756458569462\n",
      "train loss:0.017060030199561385\n",
      "train loss:0.015030626059868022\n",
      "train loss:0.032785293549766975\n",
      "train loss:0.08473621089999286\n",
      "train loss:0.017173100356382646\n",
      "train loss:0.013656022890922435\n",
      "train loss:0.026346740289258704\n",
      "train loss:0.030901250425760238\n",
      "train loss:0.04056442322699734\n",
      "train loss:0.006689541099210452\n",
      "train loss:0.026917015384523863\n",
      "train loss:0.013911931847785074\n",
      "train loss:0.04129391920307255\n",
      "train loss:0.02253481364973023\n",
      "train loss:0.01603931962814149\n",
      "train loss:0.01258669087586341\n",
      "train loss:0.027116952534477264\n",
      "train loss:0.0620200216598233\n",
      "train loss:0.01788187686676107\n",
      "train loss:0.030838655104537965\n",
      "train loss:0.09416965162227849\n",
      "train loss:0.020717139086908535\n",
      "train loss:0.04557699738196397\n",
      "train loss:0.17662505625090727\n",
      "train loss:0.03318896275778145\n",
      "train loss:0.010628526721901317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.015932304398281246\n",
      "train loss:0.019043211766018524\n",
      "train loss:0.033040717002172135\n",
      "train loss:0.04791203326679498\n",
      "train loss:0.05199916088966261\n",
      "train loss:0.021106633337455518\n",
      "train loss:0.03409499623788679\n",
      "train loss:0.005043396428806549\n",
      "train loss:0.04978491144623817\n",
      "train loss:0.041454420039031284\n",
      "train loss:0.054252932968452516\n",
      "train loss:0.016476827302783673\n",
      "train loss:0.02807258449585015\n",
      "train loss:0.047395221474518204\n",
      "train loss:0.03334613862799065\n",
      "train loss:0.026841645045167558\n",
      "train loss:0.12569351746030435\n",
      "train loss:0.03240914659689337\n",
      "train loss:0.031135018197252187\n",
      "train loss:0.008780967821110226\n",
      "train loss:0.03428356633073849\n",
      "train loss:0.022405206343839898\n",
      "train loss:0.014075760018274297\n",
      "train loss:0.1102374733717601\n",
      "train loss:0.018681436829939047\n",
      "train loss:0.07498510199576044\n",
      "train loss:0.04919718547968218\n",
      "train loss:0.06219571103662065\n",
      "train loss:0.13080490367696776\n",
      "train loss:0.05222160256827417\n",
      "train loss:0.04854911270796229\n",
      "train loss:0.03459818985491088\n",
      "train loss:0.042361033950202034\n",
      "train loss:0.014814739021003909\n",
      "train loss:0.03362743372514643\n",
      "train loss:0.007519218104299241\n",
      "train loss:0.025100564249687204\n",
      "train loss:0.04782360130405633\n",
      "train loss:0.04835067190788563\n",
      "train loss:0.012486791828221362\n",
      "train loss:0.04503856165932395\n",
      "train loss:0.06132396849327305\n",
      "train loss:0.02450505521922843\n",
      "train loss:0.008538113864878415\n",
      "train loss:0.020379094135912764\n",
      "train loss:0.015148266412604687\n",
      "train loss:0.019748222782320638\n",
      "train loss:0.08210444339197492\n",
      "train loss:0.017307407017166945\n",
      "train loss:0.0568415168042393\n",
      "train loss:0.08593891774091901\n",
      "train loss:0.032830441547393154\n",
      "train loss:0.019496949164430035\n",
      "train loss:0.07125901937038591\n",
      "train loss:0.04763225962395974\n",
      "train loss:0.04195601724743528\n",
      "train loss:0.054337601003203374\n",
      "train loss:0.072558641448611\n",
      "train loss:0.0523071671937026\n",
      "train loss:0.022573707053138988\n",
      "train loss:0.005674868340979646\n",
      "train loss:0.03434783762139319\n",
      "train loss:0.012376107659519693\n",
      "train loss:0.032085275209685744\n",
      "train loss:0.03309184639152078\n",
      "train loss:0.039639471667327675\n",
      "train loss:0.08373454329928362\n",
      "train loss:0.03826195990714055\n",
      "train loss:0.0660557984073139\n",
      "train loss:0.04368297882378916\n",
      "train loss:0.032930391859529534\n",
      "train loss:0.056562900866393415\n",
      "train loss:0.05404654376419169\n",
      "train loss:0.037923418299704116\n",
      "train loss:0.06046114265803917\n",
      "train loss:0.02943744055759987\n",
      "train loss:0.03847957136100666\n",
      "train loss:0.0515516760777823\n",
      "train loss:0.042752173352531425\n",
      "train loss:0.06963636556059541\n",
      "train loss:0.051371549992059\n",
      "train loss:0.011246284045579593\n",
      "train loss:0.038931031415404466\n",
      "train loss:0.00960201452605912\n",
      "train loss:0.011481593838485137\n",
      "train loss:0.045871555016464234\n",
      "train loss:0.025681450699263565\n",
      "train loss:0.041170397610211136\n",
      "train loss:0.012890490830642625\n",
      "train loss:0.02709311209133098\n",
      "train loss:0.05911503218830011\n",
      "train loss:0.009501497542689285\n",
      "train loss:0.016632357373821266\n",
      "train loss:0.042634375760226134\n",
      "train loss:0.03873487228335955\n",
      "train loss:0.07910591741677324\n",
      "train loss:0.01592588038245528\n",
      "train loss:0.02361530043984923\n",
      "train loss:0.0946582227522839\n",
      "train loss:0.08502450158262018\n",
      "train loss:0.017728627509239356\n",
      "train loss:0.0113804165517749\n",
      "train loss:0.04459994321407967\n",
      "train loss:0.010544566344494098\n",
      "train loss:0.01818405483133677\n",
      "train loss:0.005708623153955292\n",
      "train loss:0.010568069656893008\n",
      "train loss:0.0096127913135056\n",
      "train loss:0.0108144886818015\n",
      "train loss:0.06438773752657621\n",
      "train loss:0.018230794695529403\n",
      "train loss:0.037531153512222665\n",
      "train loss:0.05241484366033146\n",
      "train loss:0.015096221247628444\n",
      "train loss:0.027623971931434666\n",
      "train loss:0.013908686263110337\n",
      "train loss:0.013897408067599515\n",
      "train loss:0.03869900080251312\n",
      "train loss:0.009782179966497957\n",
      "train loss:0.06230830057773415\n",
      "train loss:0.0161768776016724\n",
      "train loss:0.011216493142652167\n",
      "train loss:0.0191423573324891\n",
      "train loss:0.04549368344626889\n",
      "train loss:0.04394922101234335\n",
      "train loss:0.005459463795124833\n",
      "train loss:0.03632268826578425\n",
      "train loss:0.026397712922169806\n",
      "train loss:0.09482864451299568\n",
      "train loss:0.047337045361913146\n",
      "train loss:0.013970829805872904\n",
      "train loss:0.017885975002017523\n",
      "train loss:0.014502712381403358\n",
      "train loss:0.03511238923163825\n",
      "train loss:0.017354159594950416\n",
      "train loss:0.0252323700132132\n",
      "train loss:0.0031748906198072113\n",
      "train loss:0.01630481789176368\n",
      "train loss:0.02836267747800966\n",
      "train loss:0.0166998333324149\n",
      "train loss:0.013164950854847323\n",
      "train loss:0.02576600165515625\n",
      "train loss:0.02281832603677923\n",
      "train loss:0.025912605187496506\n",
      "train loss:0.07302931297764127\n",
      "train loss:0.00979815497059612\n",
      "train loss:0.022241887106914056\n",
      "train loss:0.02298941537930186\n",
      "train loss:0.021660011568285665\n",
      "train loss:0.01305972034256299\n",
      "train loss:0.01401959108638002\n",
      "train loss:0.0152238999383476\n",
      "train loss:0.02385070197282832\n",
      "train loss:0.06192558236987323\n",
      "train loss:0.032227948268615726\n",
      "train loss:0.00934145276271851\n",
      "train loss:0.042129221499033545\n",
      "train loss:0.04498369393953696\n",
      "train loss:0.01045525545191949\n",
      "train loss:0.09220145078234959\n",
      "train loss:0.01389027433239386\n",
      "train loss:0.02177614089625132\n",
      "train loss:0.045050402003190194\n",
      "train loss:0.028088380988061993\n",
      "train loss:0.015575930024969563\n",
      "train loss:0.03899329391864308\n",
      "train loss:0.01782156838863927\n",
      "train loss:0.035725154436724235\n",
      "train loss:0.017875003951057743\n",
      "train loss:0.008441907513303471\n",
      "train loss:0.07338830035181346\n",
      "train loss:0.03868068132386027\n",
      "train loss:0.07095934847788563\n",
      "train loss:0.02279841460995507\n",
      "train loss:0.004524379578064664\n",
      "train loss:0.08796906178568475\n",
      "train loss:0.02629943631888498\n",
      "train loss:0.008700680400356235\n",
      "train loss:0.07708504790214242\n",
      "train loss:0.007721034665550805\n",
      "train loss:0.016810935158271353\n",
      "train loss:0.03735106220232163\n",
      "train loss:0.05507192499305414\n",
      "train loss:0.02534561026812243\n",
      "train loss:0.04768810333460152\n",
      "train loss:0.024493817117707987\n",
      "train loss:0.0662703536571829\n",
      "train loss:0.013149909114029024\n",
      "train loss:0.016064542810616898\n",
      "train loss:0.057571614567131384\n",
      "train loss:0.03678585379343683\n",
      "train loss:0.00955161599000717\n",
      "train loss:0.006100789356382812\n",
      "train loss:0.06840523090867848\n",
      "train loss:0.021704814860758988\n",
      "train loss:0.012398717885553577\n",
      "train loss:0.010034128761569507\n",
      "train loss:0.004816176430568484\n",
      "train loss:0.014584938577709549\n",
      "train loss:0.030693419149709564\n",
      "train loss:0.013684117250805863\n",
      "train loss:0.02195311806199785\n",
      "train loss:0.029269000230503432\n",
      "train loss:0.05356021510501379\n",
      "train loss:0.057609810482130194\n",
      "train loss:0.036773305477545946\n",
      "train loss:0.04301397638538578\n",
      "train loss:0.02793951608376382\n",
      "train loss:0.0042596949219547466\n",
      "train loss:0.032275864498247646\n",
      "train loss:0.009655386217140273\n",
      "train loss:0.03965402634235312\n",
      "train loss:0.02341246637044967\n",
      "train loss:0.0638727496934997\n",
      "train loss:0.022311019280615355\n",
      "train loss:0.0271332008139674\n",
      "train loss:0.014486506444553673\n",
      "train loss:0.01797392141544474\n",
      "train loss:0.030928748983651288\n",
      "train loss:0.031534701116269936\n",
      "train loss:0.016594721606030906\n",
      "train loss:0.00868933095143492\n",
      "train loss:0.01032108883177278\n",
      "train loss:0.04606420174612017\n",
      "train loss:0.021088361572174388\n",
      "train loss:0.010220938829682781\n",
      "train loss:0.044562988460328995\n",
      "train loss:0.029781218724017886\n",
      "train loss:0.012762505861099529\n",
      "train loss:0.10100947163356402\n",
      "train loss:0.050133725634587185\n",
      "train loss:0.03699006088562526\n",
      "train loss:0.034762957275208554\n",
      "train loss:0.01099455516723657\n",
      "train loss:0.028591922869628708\n",
      "train loss:0.01464378683371903\n",
      "train loss:0.013525530315099156\n",
      "train loss:0.020041630955741005\n",
      "train loss:0.042055437978301746\n",
      "train loss:0.003675937501892061\n",
      "train loss:0.012098495849730244\n",
      "train loss:0.022207501200734434\n",
      "train loss:0.03687693834327768\n",
      "train loss:0.059089901105656845\n",
      "train loss:0.011237030755369526\n",
      "train loss:0.053177443037059374\n",
      "train loss:0.01981055683809024\n",
      "train loss:0.03378883003385197\n",
      "train loss:0.006900489796891158\n",
      "train loss:0.035169509641514586\n",
      "train loss:0.06418352543323932\n",
      "train loss:0.011571664603128322\n",
      "train loss:0.0491402990159215\n",
      "train loss:0.012733045784355312\n",
      "train loss:0.041708717274845875\n",
      "train loss:0.020839840593559686\n",
      "train loss:0.010425061887386029\n",
      "train loss:0.019872172399427908\n",
      "train loss:0.005471358414974502\n",
      "train loss:0.02138446304066383\n",
      "train loss:0.024686847524309923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.016104162213446807\n",
      "train loss:0.02157251749903376\n",
      "train loss:0.02216594129671078\n",
      "train loss:0.030483322390849842\n",
      "train loss:0.011139014657218957\n",
      "train loss:0.03210577502061355\n",
      "train loss:0.005289679880259754\n",
      "train loss:0.017094507785049874\n",
      "train loss:0.04591672540430396\n",
      "train loss:0.015355464216352303\n",
      "train loss:0.037640076136892256\n",
      "train loss:0.03591835527988272\n",
      "train loss:0.03283953071675771\n",
      "train loss:0.13152249238273597\n",
      "train loss:0.008485231071725441\n",
      "train loss:0.02965712605882105\n",
      "train loss:0.012380438950606993\n",
      "train loss:0.015388305211218607\n",
      "train loss:0.013297443589748666\n",
      "train loss:0.04522687381419683\n",
      "train loss:0.019734276260750902\n",
      "train loss:0.009304941988339156\n",
      "train loss:0.024036249333815055\n",
      "train loss:0.017905973473614823\n",
      "train loss:0.03422305927368249\n",
      "train loss:0.046961798134745755\n",
      "train loss:0.02080914045186422\n",
      "train loss:0.007861916209133988\n",
      "train loss:0.020164452744416774\n",
      "=== epoch:5, train acc:0.985, test acc:0.981 ===\n",
      "train loss:0.011985373528235164\n",
      "train loss:0.014797145637309104\n",
      "train loss:0.023219401307910906\n",
      "train loss:0.05944750912533668\n",
      "train loss:0.011662619864029746\n",
      "train loss:0.007401962600389635\n",
      "train loss:0.05198429118130869\n",
      "train loss:0.03877782483814684\n",
      "train loss:0.02720474721185437\n",
      "train loss:0.007574381176179965\n",
      "train loss:0.025098550696825747\n",
      "train loss:0.025729195290206067\n",
      "train loss:0.06559077816443298\n",
      "train loss:0.029774982639937456\n",
      "train loss:0.02197656882269117\n",
      "train loss:0.02814147230029835\n",
      "train loss:0.06551902378813282\n",
      "train loss:0.03342019929557215\n",
      "train loss:0.011864190807720004\n",
      "train loss:0.03898545330468813\n",
      "train loss:0.020392750225037583\n",
      "train loss:0.030408340705428527\n",
      "train loss:0.020459376567060275\n",
      "train loss:0.008511396320731848\n",
      "train loss:0.021726712795381967\n",
      "train loss:0.006387439504827263\n",
      "train loss:0.02570830479409227\n",
      "train loss:0.04411652838513744\n",
      "train loss:0.03937415275113455\n",
      "train loss:0.04454511086799824\n",
      "train loss:0.05303848495418206\n",
      "train loss:0.048969687082869175\n",
      "train loss:0.020247837259255653\n",
      "train loss:0.016347243507914153\n",
      "train loss:0.009399480798954479\n",
      "train loss:0.013439584546247631\n",
      "train loss:0.006104588362977558\n",
      "train loss:0.017573279000854987\n",
      "train loss:0.06206717212276567\n",
      "train loss:0.019274838112385596\n",
      "train loss:0.006817996517523414\n",
      "train loss:0.07877441884792082\n",
      "train loss:0.00890258933821698\n",
      "train loss:0.016731451287432714\n",
      "train loss:0.016322042907693794\n",
      "train loss:0.010353796739656387\n",
      "train loss:0.0024647651262240935\n",
      "train loss:0.011092976272547956\n",
      "train loss:0.07986389442664012\n",
      "train loss:0.008159698881937065\n",
      "train loss:0.059970584707631655\n",
      "train loss:0.012561222029155121\n",
      "train loss:0.04423932909374075\n",
      "train loss:0.010103264654229864\n",
      "train loss:0.048344741186502505\n",
      "train loss:0.02194888846067043\n",
      "train loss:0.06531971230629738\n",
      "train loss:0.008925402519362476\n",
      "train loss:0.01486488856665369\n",
      "train loss:0.06545658625888208\n",
      "train loss:0.054955285565907876\n",
      "train loss:0.023400861766784374\n",
      "train loss:0.03547056362772964\n",
      "train loss:0.02405653429067904\n",
      "train loss:0.02842245600953332\n",
      "train loss:0.007194828648686753\n",
      "train loss:0.019984455645989052\n",
      "train loss:0.028631416372208646\n",
      "train loss:0.011031317778316025\n",
      "train loss:0.022198760743033376\n",
      "train loss:0.013947644917775857\n",
      "train loss:0.008330292923016094\n",
      "train loss:0.01003074321961412\n",
      "train loss:0.013932948782352524\n",
      "train loss:0.012145068678292919\n",
      "train loss:0.010757973703830333\n",
      "train loss:0.019022639296428646\n",
      "train loss:0.01877629870339382\n",
      "train loss:0.02049728938803541\n",
      "train loss:0.0028793284739662118\n",
      "train loss:0.019857581959772715\n",
      "train loss:0.038682353268729555\n",
      "train loss:0.0049045798680389515\n",
      "train loss:0.09829143279236728\n",
      "train loss:0.02515948550029032\n",
      "train loss:0.01620771080321582\n",
      "train loss:0.027389958918811713\n",
      "train loss:0.05512365675108834\n",
      "train loss:0.004759806827619761\n",
      "train loss:0.0204142733128207\n",
      "train loss:0.006680634839090296\n",
      "train loss:0.08402557415538803\n",
      "train loss:0.026768873984101535\n",
      "train loss:0.08542032540321437\n",
      "train loss:0.014342822388322414\n",
      "train loss:0.01714117136004141\n",
      "train loss:0.02527000637654532\n",
      "train loss:0.020633315263013232\n",
      "train loss:0.010875826747710777\n",
      "train loss:0.030175519237610277\n",
      "train loss:0.03675813683265885\n",
      "train loss:0.005490423828875396\n",
      "train loss:0.03693427774079268\n",
      "train loss:0.02199418670680632\n",
      "train loss:0.0053139992150977155\n",
      "train loss:0.04606724215322902\n",
      "train loss:0.03913668826483012\n",
      "train loss:0.007670590484461912\n",
      "train loss:0.0662295098965985\n",
      "train loss:0.0798297525809264\n",
      "train loss:0.010449101916291538\n",
      "train loss:0.026617197423165845\n",
      "train loss:0.01827236359110747\n",
      "train loss:0.03060011455166201\n",
      "train loss:0.01943555463398837\n",
      "train loss:0.015045640691275152\n",
      "train loss:0.06200911827756462\n",
      "train loss:0.004746387548611592\n",
      "train loss:0.023584579940342305\n",
      "train loss:0.020518306158967277\n",
      "train loss:0.037458380792419935\n",
      "train loss:0.02905506944737777\n",
      "train loss:0.010072946329244872\n",
      "train loss:0.04374749134281077\n",
      "train loss:0.09599389523982849\n",
      "train loss:0.009992557195203774\n",
      "train loss:0.011080088069153015\n",
      "train loss:0.025394195667260776\n",
      "train loss:0.004909890239994497\n",
      "train loss:0.047830527790944544\n",
      "train loss:0.03259775909514965\n",
      "train loss:0.040994193915419815\n",
      "train loss:0.051904682470645565\n",
      "train loss:0.022247378722861935\n",
      "train loss:0.01384576208044553\n",
      "train loss:0.06161355654572781\n",
      "train loss:0.055988434950447086\n",
      "train loss:0.0360502228861839\n",
      "train loss:0.056194908782579135\n",
      "train loss:0.02789085701881526\n",
      "train loss:0.011423204482068692\n",
      "train loss:0.020954576640836987\n",
      "train loss:0.020545103150650758\n",
      "train loss:0.07828794390801753\n",
      "train loss:0.010340144524746204\n",
      "train loss:0.01532874084672017\n",
      "train loss:0.032268256039394025\n",
      "train loss:0.008502758316326443\n",
      "train loss:0.005563793602943935\n",
      "train loss:0.01402765100120026\n",
      "train loss:0.026470717936692093\n",
      "train loss:0.027952415279570132\n",
      "train loss:0.08635994285344158\n",
      "train loss:0.017288487155223697\n",
      "train loss:0.012924801945611993\n",
      "train loss:0.0068241530819848125\n",
      "train loss:0.023729493722946145\n",
      "train loss:0.012184983668649545\n",
      "train loss:0.060564624298966994\n",
      "train loss:0.01951446688472944\n",
      "train loss:0.0087627451263551\n",
      "train loss:0.019426742016816646\n",
      "train loss:0.015754986657356692\n",
      "train loss:0.03341444772899837\n",
      "train loss:0.019774644396185943\n",
      "train loss:0.024311635259637625\n",
      "train loss:0.02725996976690264\n",
      "train loss:0.030951543439406072\n",
      "train loss:0.005487851542583987\n",
      "train loss:0.024496761071369156\n",
      "train loss:0.019885320376821586\n",
      "train loss:0.014494746764547794\n",
      "train loss:0.025262618589871316\n",
      "train loss:0.060429112975183996\n",
      "train loss:0.0056534000137618235\n",
      "train loss:0.028923790057771157\n",
      "train loss:0.038141204074143974\n",
      "train loss:0.029759284606928334\n",
      "train loss:0.035662365861820554\n",
      "train loss:0.04873038935978898\n",
      "train loss:0.01669463203168386\n",
      "train loss:0.018489280377587628\n",
      "train loss:0.03860654599378976\n",
      "train loss:0.021256613109942934\n",
      "train loss:0.05741122791072851\n",
      "train loss:0.07590892172347961\n",
      "train loss:0.10244134029033009\n",
      "train loss:0.0677023898677067\n",
      "train loss:0.008594634165074222\n",
      "train loss:0.006160966121718796\n",
      "train loss:0.07270537872916312\n",
      "train loss:0.015964895646463913\n",
      "train loss:0.009933458137657673\n",
      "train loss:0.019962472717457012\n",
      "train loss:0.017538996154208123\n",
      "train loss:0.06669765564297796\n",
      "train loss:0.027365927993179287\n",
      "train loss:0.07389040284805272\n",
      "train loss:0.022370023304072936\n",
      "train loss:0.009837959347506692\n",
      "train loss:0.04348929766809298\n",
      "train loss:0.033724348986145763\n",
      "train loss:0.026756621171903028\n",
      "train loss:0.046845745103671906\n",
      "train loss:0.05245808582088293\n",
      "train loss:0.036017725228781876\n",
      "train loss:0.006681126263605088\n",
      "train loss:0.017144466019397047\n",
      "train loss:0.01673971562364026\n",
      "train loss:0.051037209144002754\n",
      "train loss:0.055621249525721624\n",
      "train loss:0.07114039479644646\n",
      "train loss:0.02087546982882979\n",
      "train loss:0.006057150965660718\n",
      "train loss:0.027482393142340755\n",
      "train loss:0.044814591225552876\n",
      "train loss:0.02103707852038356\n",
      "train loss:0.036913332218748575\n",
      "train loss:0.06010171825768903\n",
      "train loss:0.00861471807114171\n",
      "train loss:0.024538668750206084\n",
      "train loss:0.00788651267932515\n",
      "train loss:0.004562198994635239\n",
      "train loss:0.01955967636692608\n",
      "train loss:0.053863543288415104\n",
      "train loss:0.018975022636089987\n",
      "train loss:0.007097437871214759\n",
      "train loss:0.02027897541478006\n",
      "train loss:0.0043195988672937195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.016035519153374284\n",
      "train loss:0.26812418394671933\n",
      "train loss:0.07344400463274896\n",
      "train loss:0.008884879929105072\n",
      "train loss:0.012685098737906419\n",
      "train loss:0.010947003285247498\n",
      "train loss:0.011846836392737562\n",
      "train loss:0.010077130303426399\n",
      "train loss:0.011261396595128594\n",
      "train loss:0.03668300978402085\n",
      "train loss:0.033050976023215065\n",
      "train loss:0.0203964549409512\n",
      "train loss:0.0028518209727547896\n",
      "train loss:0.06010165074321191\n",
      "train loss:0.00815088148946516\n",
      "train loss:0.06231997306080294\n",
      "train loss:0.04975724897757406\n",
      "train loss:0.03310728716959116\n",
      "train loss:0.004697564859667411\n",
      "train loss:0.11470109383227343\n",
      "train loss:0.004802856124237612\n",
      "train loss:0.006903806528251224\n",
      "train loss:0.01883392147157598\n",
      "train loss:0.01938472773503267\n",
      "train loss:0.0621665833630219\n",
      "train loss:0.019642645575889753\n",
      "train loss:0.0546387056817\n",
      "train loss:0.01882822773856373\n",
      "train loss:0.03878518301997468\n",
      "train loss:0.011140126750052038\n",
      "train loss:0.006829876644448386\n",
      "train loss:0.02132028285288357\n",
      "train loss:0.006574713893150487\n",
      "train loss:0.020199999762310426\n",
      "train loss:0.010747411969180198\n",
      "train loss:0.031026624149230565\n",
      "train loss:0.044360927169874484\n",
      "train loss:0.030199873237655727\n",
      "train loss:0.04727047723386574\n",
      "train loss:0.03353908711476728\n",
      "train loss:0.059076934349745924\n",
      "train loss:0.015553042350977049\n",
      "train loss:0.0025269738462362183\n",
      "train loss:0.004933229343103704\n",
      "train loss:0.07729109762883736\n",
      "train loss:0.021999082091915513\n",
      "train loss:0.012374000321110695\n",
      "train loss:0.028035276515678692\n",
      "train loss:0.027032179270264703\n",
      "train loss:0.016616351847740907\n",
      "train loss:0.01821157058967296\n",
      "train loss:0.011939070758321013\n",
      "train loss:0.012233152182312856\n",
      "train loss:0.03236726387604883\n",
      "train loss:0.013152042936544634\n",
      "train loss:0.00729339726540654\n",
      "train loss:0.020522013412060277\n",
      "train loss:0.03190779388457911\n",
      "train loss:0.008784414403158962\n",
      "train loss:0.01804305205720567\n",
      "train loss:0.026118865503832425\n",
      "train loss:0.009021812766838121\n",
      "train loss:0.01321938970482646\n",
      "train loss:0.05013331695782381\n",
      "train loss:0.005892098317727869\n",
      "train loss:0.007231165608517112\n",
      "train loss:0.03497358061776149\n",
      "train loss:0.052292981793842556\n",
      "train loss:0.017893153099457413\n",
      "train loss:0.00999664081935431\n",
      "train loss:0.016750187151274278\n",
      "train loss:0.012710598540470686\n",
      "train loss:0.06940305693757094\n",
      "train loss:0.010151995580636017\n",
      "train loss:0.005716163026847271\n",
      "train loss:0.06910479513962083\n",
      "train loss:0.044226085790633765\n",
      "train loss:0.029270943642434502\n",
      "train loss:0.024992395604675442\n",
      "train loss:0.012872426642031911\n",
      "train loss:0.040358714441262995\n",
      "train loss:0.010461660474260474\n",
      "train loss:0.025332331098787294\n",
      "train loss:0.0034811260744113774\n",
      "train loss:0.01110474415078398\n",
      "train loss:0.03465910616847884\n",
      "train loss:0.07434633834206476\n",
      "train loss:0.01003584823911621\n",
      "train loss:0.012681981010869032\n",
      "train loss:0.05091399566273181\n",
      "train loss:0.016474577907235124\n",
      "train loss:0.04471044449649056\n",
      "train loss:0.014559195087318757\n",
      "train loss:0.03219210720033689\n",
      "train loss:0.009725989674510107\n",
      "train loss:0.058652194264329216\n",
      "train loss:0.05055699291557311\n",
      "train loss:0.0333466397645724\n",
      "train loss:0.009843566599056277\n",
      "train loss:0.04093025277057959\n",
      "train loss:0.02927061742585543\n",
      "train loss:0.02445668582315548\n",
      "train loss:0.007549486137568105\n",
      "train loss:0.04441461494258079\n",
      "train loss:0.022751964834665205\n",
      "train loss:0.008453642878343074\n",
      "train loss:0.005060152118658909\n",
      "train loss:0.010576184823609176\n",
      "train loss:0.04128571795016109\n",
      "train loss:0.013828892942290281\n",
      "train loss:0.007692125283763181\n",
      "train loss:0.004455102418458692\n",
      "train loss:0.0031404403866364216\n",
      "train loss:0.03209578757947109\n",
      "train loss:0.008056296569649811\n",
      "train loss:0.01116711675671256\n",
      "train loss:0.05440585049050428\n",
      "train loss:0.056302663340410446\n",
      "train loss:0.009720978942497922\n",
      "train loss:0.01601097648219991\n",
      "train loss:0.013225329807666629\n",
      "train loss:0.00389270223222097\n",
      "train loss:0.021076177068668742\n",
      "train loss:0.025234403847160962\n",
      "train loss:0.024998027651603166\n",
      "train loss:0.01721556232578495\n",
      "train loss:0.03381410706936987\n",
      "train loss:0.017027743556384853\n",
      "train loss:0.019927559373962206\n",
      "train loss:0.021769440584814673\n",
      "train loss:0.04556655414706084\n",
      "train loss:0.0359464862355411\n",
      "train loss:0.030469812129055987\n",
      "train loss:0.021341628254021326\n",
      "train loss:0.040040066310370596\n",
      "train loss:0.018800111173997016\n",
      "train loss:0.004658330376572628\n",
      "train loss:0.012871849250427884\n",
      "train loss:0.012310338982251624\n",
      "train loss:0.021992436489317372\n",
      "train loss:0.04393541592261816\n",
      "train loss:0.04748167188411115\n",
      "train loss:0.029716846010805265\n",
      "train loss:0.17531364863520194\n",
      "train loss:0.00853024042738891\n",
      "train loss:0.04970643715423964\n",
      "train loss:0.034792588706693064\n",
      "train loss:0.006676309836397506\n",
      "train loss:0.04829289716041409\n",
      "train loss:0.0382326446232851\n",
      "train loss:0.00857927077289748\n",
      "train loss:0.03507604640823255\n",
      "train loss:0.03340849113464789\n",
      "train loss:0.003122809950227548\n",
      "train loss:0.05499298714299039\n",
      "train loss:0.020268992176197737\n",
      "train loss:0.016129503300747694\n",
      "train loss:0.008679848686085172\n",
      "train loss:0.00862987259256108\n",
      "train loss:0.02291331281902881\n",
      "train loss:0.060817691599072715\n",
      "train loss:0.021505521420384244\n",
      "train loss:0.049933706560925294\n",
      "train loss:0.007809155747593891\n",
      "train loss:0.015856064647475628\n",
      "train loss:0.010232670659989236\n",
      "train loss:0.0081873730962456\n",
      "train loss:0.03797249569664244\n",
      "train loss:0.008547810009846137\n",
      "train loss:0.01912799562070443\n",
      "train loss:0.006077005903084771\n",
      "train loss:0.0974626380069161\n",
      "train loss:0.05722674836473148\n",
      "train loss:0.019305227098496545\n",
      "train loss:0.08714059942814174\n",
      "train loss:0.015081684577924819\n",
      "train loss:0.0820363187517052\n",
      "train loss:0.08179290403558918\n",
      "train loss:0.004238013152750844\n",
      "train loss:0.021150454162554854\n",
      "train loss:0.029394644717296848\n",
      "train loss:0.016215559344963375\n",
      "train loss:0.012924691499250655\n",
      "train loss:0.036101009945488496\n",
      "train loss:0.014567845753752957\n",
      "train loss:0.002738690555631024\n",
      "train loss:0.03340928546510026\n",
      "train loss:0.022248360783243864\n",
      "train loss:0.007001616779960795\n",
      "train loss:0.0277552252822733\n",
      "train loss:0.04224342066121747\n",
      "train loss:0.016060365736432537\n",
      "train loss:0.028926888604902543\n",
      "train loss:0.0249583219685252\n",
      "train loss:0.013171942676623251\n",
      "train loss:0.021332804608004224\n",
      "train loss:0.03303710041753491\n",
      "train loss:0.0837127499826512\n",
      "train loss:0.02698600304076554\n",
      "train loss:0.008331580963055297\n",
      "train loss:0.031334515147109575\n",
      "train loss:0.007610573412678127\n",
      "train loss:0.019195031033778443\n",
      "train loss:0.016599198882011543\n",
      "train loss:0.011690252255128496\n",
      "train loss:0.057114370748322284\n",
      "train loss:0.08672921191554864\n",
      "train loss:0.009644944266252618\n",
      "train loss:0.010773834637347525\n",
      "train loss:0.03322792747702015\n",
      "train loss:0.014808855264226485\n",
      "train loss:0.01809958210470667\n",
      "train loss:0.053314020140216314\n",
      "train loss:0.005966032307028562\n",
      "train loss:0.021415873399621435\n",
      "train loss:0.038193738132593545\n",
      "train loss:0.014950143237619734\n",
      "train loss:0.014824154243461174\n",
      "train loss:0.11126990812732424\n",
      "train loss:0.036827144054688045\n",
      "train loss:0.015767749021018195\n",
      "train loss:0.010039817322882309\n",
      "train loss:0.014899375973258052\n",
      "train loss:0.01702036386006278\n",
      "train loss:0.04620216271964388\n",
      "train loss:0.0037911449180895486\n",
      "train loss:0.004659731652229068\n",
      "train loss:0.008007633493586326\n",
      "train loss:0.06304735503389987\n",
      "train loss:0.015408851023923502\n",
      "train loss:0.020954943882633605\n",
      "train loss:0.018775856635764747\n",
      "train loss:0.0734931515837079\n",
      "train loss:0.01153141182487883\n",
      "train loss:0.03299268128195337\n",
      "train loss:0.018686389970965077\n",
      "train loss:0.017543033904713415\n",
      "train loss:0.006177236348238972\n",
      "train loss:0.0060096933845266\n",
      "train loss:0.018652720736256057\n",
      "train loss:0.0204958943905659\n",
      "train loss:0.017927874581086647\n",
      "train loss:0.07372769436072579\n",
      "train loss:0.059626689844203364\n",
      "train loss:0.05600484234176232\n",
      "train loss:0.017187600425954713\n",
      "train loss:0.03343982615913459\n",
      "train loss:0.013163130325935656\n",
      "train loss:0.01732751675795729\n",
      "train loss:0.006533897550136867\n",
      "train loss:0.02215252666394442\n",
      "train loss:0.0431894640167277\n",
      "train loss:0.02146299804728688\n",
      "train loss:0.0037690542489591237\n",
      "train loss:0.06493821045567877\n",
      "train loss:0.015473327800187352\n",
      "train loss:0.010485670916840096\n",
      "train loss:0.013394085393632705\n",
      "train loss:0.004261034911228334\n",
      "train loss:0.01938580949241339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0070747419692561205\n",
      "train loss:0.07940058926475571\n",
      "train loss:0.04665726539931626\n",
      "train loss:0.06309684620888918\n",
      "train loss:0.013484856724437494\n",
      "train loss:0.025646157336789353\n",
      "train loss:0.10557149951893292\n",
      "train loss:0.034351389238254974\n",
      "train loss:0.03110746321398055\n",
      "train loss:0.01069715315266806\n",
      "train loss:0.07192504625896498\n",
      "train loss:0.004801667638470311\n",
      "train loss:0.005711691817032396\n",
      "train loss:0.013286645829696988\n",
      "train loss:0.058421682552726964\n",
      "train loss:0.06082633429885617\n",
      "train loss:0.030171735679387184\n",
      "train loss:0.03054550051066266\n",
      "train loss:0.02682498284511251\n",
      "train loss:0.03965265784613654\n",
      "train loss:0.022504383638945233\n",
      "train loss:0.011870718659785067\n",
      "train loss:0.03756150235605026\n",
      "train loss:0.0348440432848637\n",
      "train loss:0.011534111993036573\n",
      "train loss:0.030615393055822445\n",
      "train loss:0.003724272394656562\n",
      "train loss:0.04357424893615069\n",
      "train loss:0.08044441215833667\n",
      "train loss:0.009897873636709582\n",
      "train loss:0.02304286095328018\n",
      "train loss:0.00875095158342326\n",
      "train loss:0.02368042432161025\n",
      "train loss:0.015029600949141193\n",
      "train loss:0.010750549399947542\n",
      "train loss:0.007147864030018437\n",
      "train loss:0.00878461383633159\n",
      "train loss:0.042728016022708884\n",
      "train loss:0.005311786063314318\n",
      "train loss:0.014902804778448853\n",
      "train loss:0.027933461221820517\n",
      "train loss:0.007675852132233304\n",
      "train loss:0.012525498606969132\n",
      "train loss:0.007060600185719893\n",
      "train loss:0.003996330393978155\n",
      "train loss:0.03268323100284575\n",
      "train loss:0.004812891304566701\n",
      "train loss:0.01606725400992444\n",
      "train loss:0.15968514760434158\n",
      "train loss:0.004712870871232325\n",
      "train loss:0.017533431165427944\n",
      "train loss:0.012171052394399653\n",
      "train loss:0.008491732015879978\n",
      "train loss:0.027593837777013738\n",
      "train loss:0.044768403779677515\n",
      "train loss:0.008045834331079506\n",
      "train loss:0.051251508609983704\n",
      "train loss:0.012645556031926046\n",
      "train loss:0.0051213660728769905\n",
      "train loss:0.05590967925169072\n",
      "train loss:0.06646730455169479\n",
      "train loss:0.05332954641016035\n",
      "train loss:0.02145513854709361\n",
      "train loss:0.02739285533963038\n",
      "train loss:0.027746754510835542\n",
      "train loss:0.008907820097605075\n",
      "train loss:0.021170847802330865\n",
      "train loss:0.013958954666008647\n",
      "train loss:0.03124644884555802\n",
      "train loss:0.0707271866902271\n",
      "train loss:0.055517057493042296\n",
      "train loss:0.012341202622663321\n",
      "train loss:0.053304277647206535\n",
      "train loss:0.06034740430394968\n",
      "train loss:0.023333264441903374\n",
      "train loss:0.05787292721371239\n",
      "train loss:0.010348691755470256\n",
      "train loss:0.028572452543702574\n",
      "train loss:0.047959670275706356\n",
      "train loss:0.13833523227149763\n",
      "train loss:0.010274003907664975\n",
      "train loss:0.015925319513691253\n",
      "train loss:0.011439767503119593\n",
      "train loss:0.011343447892157323\n",
      "train loss:0.0043523928869035645\n",
      "train loss:0.02155026335968451\n",
      "train loss:0.02825148333280265\n",
      "train loss:0.007577284649260655\n",
      "train loss:0.0072628521124286785\n",
      "train loss:0.03961810085630152\n",
      "train loss:0.012815738814286546\n",
      "train loss:0.027852549530277017\n",
      "train loss:0.0535187258889588\n",
      "train loss:0.01994867855029401\n",
      "train loss:0.008117143893895696\n",
      "train loss:0.029308349773602207\n",
      "train loss:0.01682042038346564\n",
      "train loss:0.014780688608287898\n",
      "train loss:0.01256320813249518\n",
      "train loss:0.014270195690916745\n",
      "train loss:0.004371572219755031\n",
      "train loss:0.04805297312919404\n",
      "train loss:0.02095858523766378\n",
      "train loss:0.010588834608391908\n",
      "train loss:0.014663184527775845\n",
      "train loss:0.07945474934155126\n",
      "train loss:0.009472489976341073\n",
      "train loss:0.011067298316865853\n",
      "train loss:0.016036206559126902\n",
      "train loss:0.01054341110698736\n",
      "train loss:0.035711877947776964\n",
      "=== epoch:6, train acc:0.987, test acc:0.981 ===\n",
      "train loss:0.013828228891754099\n",
      "train loss:0.018455567038772526\n",
      "train loss:0.01788314241456869\n",
      "train loss:0.016846318836976522\n",
      "train loss:0.020257656815651538\n",
      "train loss:0.009972143079427481\n",
      "train loss:0.009394326319391076\n",
      "train loss:0.003775999027801202\n",
      "train loss:0.0026994659307500524\n",
      "train loss:0.0220897554114331\n",
      "train loss:0.017909249435055705\n",
      "train loss:0.06906856141310103\n",
      "train loss:0.005602751681039209\n",
      "train loss:0.0190065608494163\n",
      "train loss:0.0035736490349452784\n",
      "train loss:0.010570105797946538\n",
      "train loss:0.03272572261924118\n",
      "train loss:0.011460658692463307\n",
      "train loss:0.07143234877293847\n",
      "train loss:0.011925351157112456\n",
      "train loss:0.059773662529166834\n",
      "train loss:0.022012642947366632\n",
      "train loss:0.00397996686658162\n",
      "train loss:0.0073421560699754584\n",
      "train loss:0.011488822449833522\n",
      "train loss:0.017780016741632952\n",
      "train loss:0.06559529107807315\n",
      "train loss:0.020076072152579477\n",
      "train loss:0.04720272533020181\n",
      "train loss:0.002836212019269671\n",
      "train loss:0.03491912891573743\n",
      "train loss:0.008669765164333328\n",
      "train loss:0.026450790747896868\n",
      "train loss:0.004304798727110302\n",
      "train loss:0.01675328819244003\n",
      "train loss:0.00479443182425538\n",
      "train loss:0.0033865210092215185\n",
      "train loss:0.0049480023433118615\n",
      "train loss:0.10002939145272703\n",
      "train loss:0.031575608556702724\n",
      "train loss:0.009230410581577985\n",
      "train loss:0.010179371936161448\n",
      "train loss:0.006667092061532419\n",
      "train loss:0.006135422209940421\n",
      "train loss:0.02334602588378558\n",
      "train loss:0.03322893291367995\n",
      "train loss:0.033389817346612255\n",
      "train loss:0.006241697097928156\n",
      "train loss:0.03195033334309353\n",
      "train loss:0.007432676021731954\n",
      "train loss:0.013572851223764053\n",
      "train loss:0.02940879177201995\n",
      "train loss:0.005513873770434456\n",
      "train loss:0.032236544274446284\n",
      "train loss:0.02050487102635752\n",
      "train loss:0.0664423456029677\n",
      "train loss:0.03395791278456755\n",
      "train loss:0.0034268292276773363\n",
      "train loss:0.02446189299459949\n",
      "train loss:0.021603318638688793\n",
      "train loss:0.013922786206146724\n",
      "train loss:0.010646893740357231\n",
      "train loss:0.007151616844827437\n",
      "train loss:0.06351575309463452\n",
      "train loss:0.02034148802039572\n",
      "train loss:0.00425867134682268\n",
      "train loss:0.031998564718310404\n",
      "train loss:0.015636452137835245\n",
      "train loss:0.008587724523624067\n",
      "train loss:0.06483159802138784\n",
      "train loss:0.023024838875985266\n",
      "train loss:0.02336340176203749\n",
      "train loss:0.008263807505176525\n",
      "train loss:0.009657708654603036\n",
      "train loss:0.006836125936668621\n",
      "train loss:0.034093946105984\n",
      "train loss:0.03659116854640206\n",
      "train loss:0.006391628754758765\n",
      "train loss:0.024063873220833178\n",
      "train loss:0.06537661403105728\n",
      "train loss:0.05492738524196717\n",
      "train loss:0.013026399332272882\n",
      "train loss:0.015703309983282128\n",
      "train loss:0.03985791595766448\n",
      "train loss:0.15712772955295526\n",
      "train loss:0.012518358490612037\n",
      "train loss:0.028390309712718043\n",
      "train loss:0.037213903151575994\n",
      "train loss:0.02156222107384154\n",
      "train loss:0.027142251863363973\n",
      "train loss:0.031204622652025404\n",
      "train loss:0.01255125450954875\n",
      "train loss:0.007100605383985329\n",
      "train loss:0.027636473223577762\n",
      "train loss:0.03041116870820956\n",
      "train loss:0.047222095152255346\n",
      "train loss:0.0063957767000739065\n",
      "train loss:0.006469552041630009\n",
      "train loss:0.02144577834814213\n",
      "train loss:0.014431193086188931\n",
      "train loss:0.021009122106409213\n",
      "train loss:0.08326801833360792\n",
      "train loss:0.003392808941733585\n",
      "train loss:0.02435712334967548\n",
      "train loss:0.018808898029899424\n",
      "train loss:0.021590502548198814\n",
      "train loss:0.029754058201423784\n",
      "train loss:0.02851849587587806\n",
      "train loss:0.008827190080301719\n",
      "train loss:0.03584873249055608\n",
      "train loss:0.01806385605792599\n",
      "train loss:0.018393399643938643\n",
      "train loss:0.01667825933050751\n",
      "train loss:0.004837757612447501\n",
      "train loss:0.04287847636200777\n",
      "train loss:0.01581679117684814\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from ch07.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
